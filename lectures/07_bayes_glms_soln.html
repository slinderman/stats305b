
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Bayesian GLMs &#8212; STATS 305B: Models and Algorithms for Discrete Data</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="../_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=888ff710"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"mba": "\\boldsymbol{a}", "mbb": "\\boldsymbol{b}", "mbc": "\\boldsymbol{c}", "mbd": "\\boldsymbol{d}", "mbe": "\\boldsymbol{e}", "mbf": "\\boldsymbol{f}", "mbg": "\\boldsymbol{g}", "mbh": "\\boldsymbol{h}", "mbi": "\\boldsymbol{i}", "mbj": "\\boldsymbol{j}", "mbk": "\\boldsymbol{k}", "mbl": "\\boldsymbol{l}", "mbm": "\\boldsymbol{m}", "mbn": "\\boldsymbol{n}", "mbo": "\\boldsymbol{o}", "mbp": "\\boldsymbol{p}", "mbq": "\\boldsymbol{q}", "mbr": "\\boldsymbol{r}", "mbs": "\\boldsymbol{s}", "mbt": "\\boldsymbol{t}", "mbu": "\\boldsymbol{u}", "mbv": "\\boldsymbol{v}", "mbw": "\\boldsymbol{w}", "mbx": "\\boldsymbol{x}", "mby": "\\boldsymbol{y}", "mbz": "\\boldsymbol{z}", "mbA": "\\boldsymbol{A}", "mbB": "\\boldsymbol{B}", "mbC": "\\boldsymbol{C}", "mbD": "\\boldsymbol{D}", "mbE": "\\boldsymbol{E}", "mbF": "\\boldsymbol{F}", "mbG": "\\boldsymbol{G}", "mbH": "\\boldsymbol{H}", "mbI": "\\boldsymbol{I}", "mbJ": "\\boldsymbol{J}", "mbK": "\\boldsymbol{K}", "mbL": "\\boldsymbol{L}", "mbM": "\\boldsymbol{M}", "mbN": "\\boldsymbol{N}", "mbO": "\\boldsymbol{O}", "mbP": "\\boldsymbol{P}", "mbQ": "\\boldsymbol{Q}", "mbR": "\\boldsymbol{R}", "mbS": "\\boldsymbol{S}", "mbT": "\\boldsymbol{T}", "mbU": "\\boldsymbol{U}", "mbV": "\\boldsymbol{V}", "mbW": "\\boldsymbol{W}", "mbX": "\\boldsymbol{X}", "mbY": "\\boldsymbol{Y}", "mbZ": "\\boldsymbol{Z}", "bbA": "\\mathbb{A}", "bbB": "\\mathbb{B}", "bbC": "\\mathbb{C}", "bbD": "\\mathbb{D}", "bbE": "\\mathbb{E}", "bbG": "\\mathbb{G}", "bbH": "\\mathbb{H}", "bbI": "\\mathbb{I}", "bbJ": "\\mathbb{J}", "bbK": "\\mathbb{K}", "bbL": "\\mathbb{L}", "bbM": "\\mathbb{M}", "bbN": "\\mathbb{N}", "bbO": "\\mathbb{O}", "bbP": "\\mathbb{P}", "bbQ": "\\mathbb{Q}", "bbR": "\\mathbb{R}", "bbS": "\\mathbb{S}", "bbT": "\\mathbb{T}", "bbU": "\\mathbb{U}", "bbV": "\\mathbb{V}", "bbW": "\\mathbb{W}", "bbX": "\\mathbb{X}", "bbY": "\\mathbb{Y}", "bbZ": "\\mathbb{Z}", "cA": "\\mathcal{A}", "cB": "\\mathcal{B}", "cC": "\\mathcal{C}", "cD": "\\mathcal{D}", "cE": "\\mathcal{E}", "cG": "\\mathcal{G}", "cH": "\\mathcal{H}", "cI": "\\mathcal{I}", "cJ": "\\mathcal{J}", "cK": "\\mathcal{K}", "cL": "\\mathcal{L}", "cM": "\\mathcal{M}", "cN": "\\mathcal{N}", "cO": "\\mathcal{O}", "cP": "\\mathcal{P}", "cQ": "\\mathcal{Q}", "cR": "\\mathcal{R}", "cS": "\\mathcal{S}", "cT": "\\mathcal{T}", "cU": "\\mathcal{U}", "cV": "\\mathcal{V}", "cW": "\\mathcal{W}", "cX": "\\mathcal{X}", "cY": "\\mathcal{Y}", "cZ": "\\mathcal{Z}", "mbalpha": "\\boldsymbol{\\alpha}", "mbbeta": "\\boldsymbol{\\beta}", "mbgamma": "\\boldsymbol{\\gamma}", "mbdelta": "\\boldsymbol{\\delta}", "mbepsilon": "\\boldsymbol{\\epsilon}", "mbchi": "\\boldsymbol{\\chi}", "mbeta": "\\boldsymbol{\\eta}", "mbiota": "\\boldsymbol{\\iota}", "mbkappa": "\\boldsymbol{\\kappa}", "mblambda": "\\boldsymbol{\\lambda}", "mbmu": "\\boldsymbol{\\mu}", "mbnu": "\\boldsymbol{\\nu}", "mbomega": "\\boldsymbol{\\omega}", "mbtheta": "\\boldsymbol{\\theta}", "mbphi": "\\boldsymbol{\\phi}", "mbpi": "\\boldsymbol{\\pi}", "mbpsi": "\\boldsymbol{\\psi}", "mbrho": "\\boldsymbol{\\rho}", "mbsigma": "\\boldsymbol{\\sigma}", "mbtau": "\\boldsymbol{\\tau}", "mbupsilon": "\\boldsymbol{\\upsilon}", "mbxi": "\\boldsymbol{\\xi}", "mbzeta": "\\boldsymbol{\\zeta}", "mbvarepsilon": "\\boldsymbol{\\varepsilon}", "mbvarphi": "\\boldsymbol{\\varphi}", "mbvartheta": "\\boldsymbol{\\vartheta}", "mbvarrho": "\\boldsymbol{\\varrho}", "mbDelta": "\\boldsymbol{\\Delta}", "mbGamma": "\\boldsymbol{\\Gamma}", "mbLambda": "\\boldsymbol{\\Lambda}", "mbOmega": "\\boldsymbol{\\Omega}", "mbPhi": "\\boldsymbol{\\Phi}", "mbPsi": "\\boldsymbol{\\Psi}", "mbPi": "\\boldsymbol{\\Pi}", "mbSigma": "\\boldsymbol{\\Sigma}", "mbTheta": "\\boldsymbol{\\Theta}", "mbUpsilon": "\\boldsymbol{\\Upsilon}", "mbXi": "\\boldsymbol{\\Xi}", "mbzero": "\\boldsymbol{0}", "mbone": "\\boldsymbol{1}", "iid": ["\\stackrel{\\text{iid}}{#1}", 1], "ind": ["\\stackrel{\\text{ind}}{#1}", 1], "dif": "\\mathop{}\\!\\mathrm{d}", "diag": "\\textrm{diag}", "supp": "\\textrm{supp}", "Tr": "\\textrm{Tr}", "E": "\\mathbb{E}", "Var": "\\textrm{Var}", "Cov": "\\textrm{Cov}", "reals": "\\mathbb{R}", "naturals": "\\mathbb{N}", "KL": ["D_{\\textrm{KL}}\\left(#1\\;\\|\\;#2\\right)", 2]}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lectures/07_bayes_glms_soln';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Sparse GLMs" href="08_sparse_glms.html" />
    <link rel="prev" title="Bayesian Inference" href="06_bayes.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
  
    <p class="title logo__title">STATS 305B: Models and Algorithms for Discrete Data</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Overview
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Notes</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01_distributions.html">Discrete Distributions and the Basics of Statistical Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_contingency_tables.html">Contingency Tables</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_logreg.html">Logistic Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="04_expfam.html">Exponential Families</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_glms.html">Generalized Linear Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="06_bayes.html">Bayesian Inference</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Bayesian GLMs</a></li>
<li class="toctree-l1"><a class="reference internal" href="08_sparse_glms.html">Sparse GLMs</a></li>
<li class="toctree-l1"><a class="reference internal" href="09_mixtures.html">Mixture Models and EM</a></li>
<li class="toctree-l1"><a class="reference internal" href="10_hmms.html">Hidden Markov Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="11_vaes.html">Variational Autoencoders</a></li>
<li class="toctree-l1"><a class="reference internal" href="11_vaes_demo.html">Demo: Neural Networks and VAEs</a></li>
<li class="toctree-l1"><a class="reference internal" href="12_rnns.html">Recurrent Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="13_transformers.html">Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="15_graphs.html">Random Graphs Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="16_diffusion.html">Denoising Diffusion Models</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Assignments</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../assignments/hw0/hw0.html">HW0: PyTorch Primer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../assignments/hw1/hw1.html">HW1: Logistic Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../assignments/hw2/hw2.html">HW2: Bayesian GLMs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../assignments/hw3/hw3.html">HW3: Hidden Markov Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../assignments/hw4/hw4.html">HW4: Large Language Models</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">References</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="99_references.html">References</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/slinderman/stats305b/blob/winter2024/lectures/07_bayes_glms_soln.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/lectures/07_bayes_glms_soln.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Bayesian GLMs</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#setup">Setup</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-logistic-regression">Bayesian Logistic Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model">Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#generate-synthetic-data">Generate Synthetic Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#log-probability">Log Probability</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#metropolis-hastings">Metropolis-Hastings</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#implement-a-single-rwmh-step">Implement a single RWMH step</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#implement-the-mcmc-loop">Implement the MCMC loop</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#let-er-rip">Let ‘er rip!</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#plot-the-results">Plot the results</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise">Exercise</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#robust-bayesian-linear-regression">Robust Bayesian Linear Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#primer-bayesian-linear-regression-with-heteroskedastic-noise">Primer: Bayesian Linear Regression with Heteroskedastic Noise</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Exercise</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#solution">Solution</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#robust-regression-with-students-t-noise">Robust Regression with Students’ T Noise</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#student-s-t-as-a-scale-mixture-of-gaussians">Student’s t as a scale-mixture of Gaussians</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#augmentation-tricks">Augmentation Tricks</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Exercise</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Solution</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Generate Synthetic Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implement-a-gibbs-sampler">Implement a Gibbs sampler</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Let ‘er rip!</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Plot the results</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-probit-regression">Bayesian Probit Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">Exercise</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#augmentation-schemes-for-bayesian-logistic-regression">Augmentation Schemes for Bayesian Logistic Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="bayesian-glms">
<h1>Bayesian GLMs<a class="headerlink" href="#bayesian-glms" title="Link to this heading">#</a></h1>
<p>We’ve taken our first steps toward becoming Bayesian statisticians! We learned about conjugate priors, which permit exact inference for simple probabilistic models like the beta-Bernoulli model. However, we found that more complex models require some form of approximate posterior inference. MCMC methods offer a general-purpose solution to approximate inference, allowing us to construct Markov chains for which the stationary distribution is the posterior distribution of interest. Now, let’s put those ideas into practice by developing MCMC algorithms for Bayesian GLMs.</p>
<section id="setup">
<h2>Setup<a class="headerlink" href="#setup" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="n">capture</span>
<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">jaxtyping</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">fastprogress</span> <span class="kn">import</span> <span class="n">progress_bar</span>

<span class="kn">from</span> <span class="nn">jaxtyping</span> <span class="kn">import</span> <span class="n">Array</span><span class="p">,</span> <span class="n">Float</span>
<span class="kn">from</span> <span class="nn">mpl_toolkits.axes_grid1</span> <span class="kn">import</span> <span class="n">make_axes_locatable</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Tuple</span>
<span class="kn">from</span> <span class="nn">torch.distributions</span> <span class="kn">import</span> <span class="n">Bernoulli</span><span class="p">,</span> <span class="n">Gamma</span><span class="p">,</span> <span class="n">MultivariateNormal</span><span class="p">,</span> <span class="n">Normal</span><span class="p">,</span> <span class="n">StudentT</span><span class="p">,</span> \
    <span class="n">Uniform</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="bayesian-logistic-regression">
<h2>Bayesian Logistic Regression<a class="headerlink" href="#bayesian-logistic-regression" title="Link to this heading">#</a></h2>
<section id="model">
<h3>Model<a class="headerlink" href="#model" title="Link to this heading">#</a></h3>
<p>Let <span class="math notranslate nohighlight">\(y_i \in \{0,1\}\)</span> denote a binary observation and <span class="math notranslate nohighlight">\(x_i \in \mathbb{R}^p\)</span> a corresponding vector of covariates. Let <span class="math notranslate nohighlight">\(\beta \in \mathbb{R}^p\)</span> denote the weights of our GLM. Our probabilistic model is,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
p(y, \beta \mid X)
&amp;= p(\beta) \prod_{i=1}^n p(y_i \mid \beta, x_i) \\
&amp;= \mathrm{N}(\beta \mid 0, \gamma^{-1} I) \prod_{i=1}^n \mathrm{Bern}(y_i \mid f(x_i^\top \beta))
\end{align*}\]</div>
<p>where <span class="math notranslate nohighlight">\(f: \mathbb{R} \mapsto (0,1)\)</span> is the mean function. The canonical mean function is <span class="math notranslate nohighlight">\(f(a) = \sigma(a) = 1 / (1+e^{-a})\)</span>, the logistic function, but we will consider other link functions in this lecture as well.</p>
<p><strong>Our goal</strong> is to approximate the posterior distribution <span class="math notranslate nohighlight">\(p(\beta \mid y, X)\)</span>. Unfortuantely, the posterior doesn’t have a simple closed form since the Bernoulli likelihood is not conjugate with the Gaussian prior. Instead, we will develop MCMC algorithms to draw samples from the posterior distribution, at least asymptotically.</p>
</section>
<section id="generate-synthetic-data">
<h3>Generate Synthetic Data<a class="headerlink" href="#generate-synthetic-data" title="Link to this heading">#</a></h3>
<p>Let’s start by generating some synthetic data from a Bernoulli GLM with true weights <span class="math notranslate nohighlight">\(\beta^\star\)</span>. To make it easy to visualize, we’ll use two dimensional features, so our weights will be three dimensional when we include an intercept.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">305</span> <span class="o">+</span> <span class="nb">ord</span><span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">))</span>

<span class="c1"># Set constants</span>
<span class="n">num_features</span> <span class="o">=</span> <span class="mi">3</span>                <span class="c1"># first &quot;feature&quot; is all ones</span>
<span class="n">num_data</span> <span class="o">=</span> <span class="mi">100</span>                  <span class="c1"># number of data points to sample</span>
<span class="n">mean_func</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span>       <span class="c1"># canonical mean function</span>
<span class="n">prior_scale</span> <span class="o">=</span> <span class="mf">3.</span>                <span class="c1"># std of Gaussian prior \gamma^{-1/2}</span>

<span class="c1"># Fix true weights. Sample features, and responses.</span>
<span class="n">true_weights</span> <span class="o">=</span> <span class="mf">1.5</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">])</span>

<span class="n">features</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">num_data</span><span class="p">),</span>
    <span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="n">num_data</span><span class="p">,</span> <span class="n">num_features</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
<span class="p">])</span>
<span class="n">responses</span> <span class="o">=</span> <span class="n">Bernoulli</span><span class="p">(</span><span class="n">mean_func</span><span class="p">(</span><span class="n">features</span> <span class="o">@</span> <span class="n">true_weights</span><span class="p">))</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_sample</span><span class="p">(</span><span class="n">responses</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;num_data&quot;</span><span class="p">],</span>
                <span class="n">features</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;num_data num_features&quot;</span><span class="p">],</span>
                <span class="n">weights</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;num_features&quot;</span><span class="p">],</span>
                <span class="n">mean_func</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
                <span class="n">lim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
                <span class="n">num_points</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
                <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Helper function to plot the samples and weights of a Bernoulli GLM.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">*</span><span class="n">lim</span><span class="p">,</span> <span class="n">num_points</span><span class="p">),</span>
                            <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">*</span><span class="n">lim</span><span class="p">,</span> <span class="n">num_points</span><span class="p">))</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">num_points</span><span class="o">**</span><span class="mi">2</span><span class="p">),</span>
        <span class="n">x1</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span>
        <span class="n">x2</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
        <span class="p">])</span>
    <span class="n">probs</span> <span class="o">=</span> <span class="n">mean_func</span><span class="p">(</span><span class="n">X</span> <span class="o">@</span> <span class="n">weights</span><span class="p">)</span>

    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="n">figsize</span><span class="p">)</span>
    <span class="n">im</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">probs</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">num_points</span><span class="p">,</span> <span class="n">num_points</span><span class="p">)),</span>
                    <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;Greys&quot;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=-</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span>
                    <span class="n">extent</span><span class="o">=</span><span class="n">lim</span> <span class="o">+</span> <span class="nb">tuple</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">lim</span><span class="p">)))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">features</span><span class="p">[</span><span class="n">responses</span><span class="o">==</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">features</span><span class="p">[</span><span class="n">responses</span><span class="o">==</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
             <span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;$y=1$&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">features</span><span class="p">[</span><span class="n">responses</span><span class="o">==</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">features</span><span class="p">[</span><span class="n">responses</span><span class="o">==</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
             <span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;$y=0$&quot;</span><span class="p">)</span>

    <span class="c1"># Draw the true decision boundary</span>
    <span class="n">x1s</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">*</span><span class="n">lim</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
    <span class="n">x2s</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">weights</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">x1s</span><span class="p">)</span> <span class="o">/</span> <span class="n">weights</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x1s</span><span class="p">,</span> <span class="n">x2s</span><span class="p">,</span> <span class="s1">&#39;:k&#39;</span><span class="p">)</span>

    <span class="c1"># Clean up the axes</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$x_1$&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$x_2$&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">lim</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="n">lim</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

    <span class="c1"># Colorbar</span>
    <span class="n">divider</span> <span class="o">=</span> <span class="n">make_axes_locatable</span><span class="p">(</span><span class="n">ax</span><span class="p">)</span>
    <span class="n">cax</span> <span class="o">=</span> <span class="n">divider</span><span class="o">.</span><span class="n">new_horizontal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="s2">&quot;5%&quot;</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">add_axes</span><span class="p">(</span><span class="n">cax</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">im</span><span class="p">,</span> <span class="n">cax</span><span class="o">=</span><span class="n">cax</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;probability&quot;</span><span class="p">)</span>

<span class="n">plot_sample</span><span class="p">(</span><span class="n">responses</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">true_weights</span><span class="p">,</span> <span class="n">mean_func</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
</pre></div>
</div>
<img alt="../_images/3c281037af363793054f5dbe809672f52344082da3b5f2966587533c1395c3c6.png" src="../_images/3c281037af363793054f5dbe809672f52344082da3b5f2966587533c1395c3c6.png" />
</div>
</div>
</section>
<section id="log-probability">
<h3>Log Probability<a class="headerlink" href="#log-probability" title="Link to this heading">#</a></h3>
<p>Write a function to compute the log joint probability, <span class="math notranslate nohighlight">\(p(y, \beta \mid X)\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">log_joint</span><span class="p">(</span><span class="n">responses</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;num_data&quot;</span><span class="p">],</span>
              <span class="n">features</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;num_data num_features&quot;</span><span class="p">],</span>
              <span class="n">weights</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;num_features&quot;</span><span class="p">],</span>
              <span class="n">mean_func</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
              <span class="n">prior_scale</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Compute the log joint probability of the responses and</span>
<span class="sd">    weights given the features. Assume the weights have a Gaussian</span>
<span class="sd">    prior with mean zero and the specified scale.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    Log joint probability of the responses and weights.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">lp</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">prior_scale</span><span class="p">)</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="n">lp</span> <span class="o">+=</span> <span class="n">Bernoulli</span><span class="p">(</span><span class="n">mean_func</span><span class="p">(</span><span class="n">features</span> <span class="o">@</span> <span class="n">weights</span><span class="p">))</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">responses</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">lp</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="metropolis-hastings">
<h3>Metropolis-Hastings<a class="headerlink" href="#metropolis-hastings" title="Link to this heading">#</a></h3>
<p>First, let’s implement a simple Metropolis-Hasting (MH) algorithm with a symmetric Gaussian proposal distribution,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
q(\beta' \mid \beta) &amp;= \mathrm{N}(\beta' \mid \beta, \nu^2 I)
\end{align*}\]</div>
<p>where <span class="math notranslate nohighlight">\(\nu\)</span> is the standard deviation of the proposal. This is called <strong>random walk Metropolis-Hastings (RWMH)</strong>.</p>
<section id="implement-a-single-rwmh-step">
<h4>Implement a single RWMH step<a class="headerlink" href="#implement-a-single-rwmh-step" title="Link to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">rwmh_step</span><span class="p">(</span><span class="n">log_prob</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
              <span class="n">curr_weights</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;num_features&quot;</span><span class="p">],</span>
              <span class="n">curr_log_prob</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
              <span class="n">proposal_scale</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> \
              <span class="n">Tuple</span><span class="p">[</span><span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;num_features&quot;</span><span class="p">],</span> <span class="nb">float</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Perform one step of random walk Metropolis-Hastings starting</span>
<span class="sd">    from the current weights and using a symmetric Gaussian proposal</span>
<span class="sd">    with the specified scale.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    log_prob: a function that takes in weights and outputs an unnormalized</span>
<span class="sd">        log density. In our case, this function will return the log joint</span>
<span class="sd">        probability of the weights and responses. As a function of the weights,</span>
<span class="sd">        this is equal to the log posterior probability up to an unknown</span>
<span class="sd">        normalizing constant.</span>
<span class="sd">    curr_weights: the current parameters</span>
<span class="sd">    curr_log_prob: the current log probability (so we don&#39;t have to re-evaluate)</span>
<span class="sd">    proposal_scale: the standard deviation of the spherical Gaussian proposal</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    Next sample of the weights and the corresponding log prob</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">prop_weights</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="n">curr_weights</span><span class="p">,</span> <span class="n">proposal_scale</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
    <span class="n">prop_log_prob</span> <span class="o">=</span> <span class="n">log_prob</span><span class="p">(</span><span class="n">prop_weights</span><span class="p">)</span>
    <span class="n">accept</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">Uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">())</span> <span class="o">&lt;</span> <span class="n">prop_log_prob</span> <span class="o">-</span> <span class="n">curr_log_prob</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">prop_weights</span><span class="p">,</span> <span class="n">prop_log_prob</span><span class="p">)</span> <span class="k">if</span> <span class="n">accept</span> <span class="k">else</span> <span class="p">(</span><span class="n">curr_weights</span><span class="p">,</span> <span class="n">curr_log_prob</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="implement-the-mcmc-loop">
<h4>Implement the MCMC loop<a class="headerlink" href="#implement-the-mcmc-loop" title="Link to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">rwmh</span><span class="p">(</span><span class="n">log_prob</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
         <span class="n">init_weights</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;num_features&quot;</span><span class="p">],</span>
         <span class="n">proposal_scale</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
         <span class="n">num_iters</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">,</span>
         <span class="p">)</span> <span class="o">-&gt;</span> \
         <span class="n">Tuple</span><span class="p">[</span><span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;num_samples num_features&quot;</span><span class="p">],</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;num_samples&quot;</span><span class="p">]]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Run the random walk MH algorithm from the given starting point and for the</span>
<span class="sd">    specified number of iterations.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    Array of weight samples and corresponding log joint probabilities.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Initialize the loop</span>
    <span class="n">curr_weights</span> <span class="o">=</span> <span class="n">init_weights</span>
    <span class="n">curr_log_prob</span> <span class="o">=</span> <span class="n">log_prob</span><span class="p">(</span><span class="n">curr_weights</span><span class="p">)</span>
    <span class="n">weight_samples</span> <span class="o">=</span> <span class="p">[</span><span class="n">curr_weights</span><span class="p">]</span>
    <span class="n">log_probs</span> <span class="o">=</span> <span class="p">[</span><span class="n">curr_log_prob</span><span class="p">]</span>

    <span class="k">for</span> <span class="n">itr</span> <span class="ow">in</span> <span class="n">progress_bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">num_iters</span><span class="p">)):</span>
        <span class="n">curr_weights</span><span class="p">,</span> <span class="n">curr_log_prob</span> <span class="o">=</span> <span class="n">rwmh_step</span><span class="p">(</span><span class="n">log_prob</span><span class="p">,</span>
                                                <span class="n">curr_weights</span><span class="p">,</span>
                                                <span class="n">curr_log_prob</span><span class="p">,</span>
                                                <span class="n">proposal_scale</span><span class="p">)</span>
        <span class="n">weight_samples</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">curr_weights</span><span class="p">)</span>
        <span class="n">log_probs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">curr_log_prob</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">weight_samples</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">log_probs</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="let-er-rip">
<h3>Let ‘er rip!<a class="headerlink" href="#let-er-rip" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Make a log prob function that closes over the observations and hyperparams</span>
<span class="n">log_prob</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">weights</span><span class="p">:</span> <span class="n">log_joint</span><span class="p">(</span><span class="n">responses</span><span class="p">,</span>
                                     <span class="n">features</span><span class="p">,</span>
                                     <span class="n">weights</span><span class="p">,</span>
                                     <span class="n">mean_func</span><span class="p">,</span>
                                     <span class="n">prior_scale</span><span class="p">)</span>

<span class="c1"># Run RWMH starting from \beta = 0</span>
<span class="n">init_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_features</span><span class="p">)</span>
<span class="n">weight_samples</span><span class="p">,</span> <span class="n">log_probs</span> <span class="o">=</span> <span class="n">rwmh</span><span class="p">(</span><span class="n">log_prob</span><span class="p">,</span> <span class="n">init_weights</span><span class="p">,</span> <span class="n">proposal_scale</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">
<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div><div class="output text_html">
    <div>
      <progress value='10000' class='' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>
      100.00% [10000/10000 00:25&lt;00:00]
    </div>
    </div></div>
</div>
</section>
<section id="plot-the-results">
<h3>Plot the results<a class="headerlink" href="#plot-the-results" title="Link to this heading">#</a></h3>
<p>First look at the log joint probability as a function of MCMC iteration. It should go up and then stabilize. How does it compare to the log joint probability evaluated at the true weights, <span class="math notranslate nohighlight">\(\beta^\star\)</span>?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">log_probs</span> <span class="o">/</span> <span class="n">num_data</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">log_prob</span><span class="p">(</span><span class="n">true_weights</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_data</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;iteration&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;scaled log joint probability&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0, 0.5, &#39;scaled log joint probability&#39;)
</pre></div>
</div>
<img alt="../_images/6cc1928bb47204b33c8b853614da3c98b921236350cf9643f95645cd891f0f1d.png" src="../_images/6cc1928bb47204b33c8b853614da3c98b921236350cf9643f95645cd891f0f1d.png" />
</div>
</div>
<p>Now make a <strong>trace plot</strong> of the weights acros MCMC iteration.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">num_features</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">width_ratios</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_features</span><span class="p">):</span>
    <span class="c1"># Plot the trace of the samples</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">weight_samples</span><span class="p">[:,</span> <span class="n">j</span><span class="p">])</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">true_weights</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\beta_</span><span class="si">{}</span><span class="s2">$&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">j</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">num_features</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span> <span class="n">axs</span><span class="p">[</span><span class="n">j</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([])</span>

    <span class="c1"># Plot a histogram of samples</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">weight_samples</span><span class="p">[:,</span> <span class="n">j</span><span class="p">],</span> <span class="mi">20</span><span class="p">,</span> <span class="n">orientation</span><span class="o">=</span><span class="s2">&quot;horizontal&quot;</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">true_weights</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">)</span>

<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;trace plot&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;marginal&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;iteration&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/5e79eaddf547b6585e2f941d14defb728f97a84a7d098833af7dde6d92fb769b.png" src="../_images/5e79eaddf547b6585e2f941d14defb728f97a84a7d098833af7dde6d92fb769b.png" />
</div>
</div>
<p>Finally, let’s approximate the posterior mean, <span class="math notranslate nohighlight">\(\bar{\beta}\)</span>, and plot the data and predicted probabilities under <span class="math notranslate nohighlight">\(\bar{\beta}\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">mean_weights</span> <span class="o">=</span> <span class="n">weight_samples</span><span class="p">[</span><span class="mi">100</span><span class="p">:]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plot_sample</span><span class="p">(</span><span class="n">responses</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">mean_weights</span><span class="p">,</span> <span class="n">mean_func</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Posterior Mean Weights&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/2d80f07ee6dc6281331401a71d88cf00792b46be548eb6ad9bef8773761e122f.png" src="../_images/2d80f07ee6dc6281331401a71d88cf00792b46be548eb6ad9bef8773761e122f.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plot_sample</span><span class="p">(</span><span class="n">responses</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">true_weights</span><span class="p">,</span> <span class="n">mean_func</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;True Weights&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/6220dbfb3574adc2bd43053e1fcc6e812a49712dca8631be86dc2faec971dfa3.png" src="../_images/6220dbfb3574adc2bd43053e1fcc6e812a49712dca8631be86dc2faec971dfa3.png" />
</div>
</div>
<p>Finally, let’s look at the MCMC estimate of the posterior <em>pairwise</em> marginal <span class="math notranslate nohighlight">\(p(\beta_1, \beta_2 \mid y, X)\)</span> to see how correlated they are.</p>
<p><strong>Question</strong>: Before running the cell below, do you expect the weights to be correlated under the posterior? Explain your answer.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">weight_samples</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">weight_samples</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;$\beta$ samples&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">true_weights</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">true_weights</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="s1">&#39;r*&#39;</span><span class="p">,</span> <span class="n">ms</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">mew</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;$\beta^*$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\beta_1$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\beta_2$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/bbfcaa68afdfd875cb9d60c2077f856c2451661407d84e3df8607d9429345c0c.png" src="../_images/bbfcaa68afdfd875cb9d60c2077f856c2451661407d84e3df8607d9429345c0c.png" />
</div>
</div>
</section>
<section id="exercise">
<h3>Exercise<a class="headerlink" href="#exercise" title="Link to this heading">#</a></h3>
<p>Now go back and play with the hyperparameters.</p>
<ul class="simple">
<li><p>What if you make the true weights larger — is the posterior more concentrated around the true weights?</p></li>
<li><p>What happens if you change the prior variance?</p></li>
<li><p>What happens if you change the proposal variance?</p></li>
</ul>
</section>
</section>
<section id="robust-bayesian-linear-regression">
<h2>Robust Bayesian Linear Regression<a class="headerlink" href="#robust-bayesian-linear-regression" title="Link to this heading">#</a></h2>
<p>Now let’s go on a little side quest and talk about robust linear regression. It will be a good excuse to introduce the concept of <em>augmentation</em> and implement a Gibbs sampling algorithm. Then we’ll come back to Bayesian GLMs and apply the same ideas.</p>
<section id="primer-bayesian-linear-regression-with-heteroskedastic-noise">
<h3>Primer: Bayesian Linear Regression with Heteroskedastic Noise<a class="headerlink" href="#primer-bayesian-linear-regression-with-heteroskedastic-noise" title="Link to this heading">#</a></h3>
<p>Suppose you have observations <span class="math notranslate nohighlight">\(y_i \in \mathbb{R}\)</span> and features <span class="math notranslate nohighlight">\(x_i \in \mathbb{R}^p\)</span>. Unlike the ordinary linear regression setup, assume you have <em>heteroskedastic</em> noise with known variance, <span class="math notranslate nohighlight">\(\sigma_i^2\)</span> for data points <span class="math notranslate nohighlight">\(i=1,\ldots, n\)</span>.</p>
<p>The joint probability is,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
p(y, \beta \mid X, \sigma^2)
&amp;= p(\beta) \prod_{i=1}^n p(y_i \mid \beta, x_i, \sigma_i^2) \\
&amp;= \mathrm{N}(\beta \mid 0, \gamma^{-1} I) \prod_{i=1}^n \mathrm{N}(y_i \mid \beta^\top x_i, \sigma_i^2)
\end{align*}\]</div>
</section>
<section id="id1">
<h3>Exercise<a class="headerlink" href="#id1" title="Link to this heading">#</a></h3>
<p>Derive the posterior distribution <span class="math notranslate nohighlight">\(p(\beta \mid y, X, \sigma^2)\)</span>.</p>
<section id="solution">
<h4>Solution<a class="headerlink" href="#solution" title="Link to this heading">#</a></h4>
<div class="dropdown admonition">
<p class="admonition-title">Solution</p>
<p>The posterior is proportional to the joint probability,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
p(\beta \mid y, X, \sigma^2)
&amp;\propto \mathrm{N}(\beta \mid 0, \gamma^{-1} I) \prod_{i=1}^n \mathrm{N}(y_i \mid \beta^\top x_i, \sigma_i^2) \\
&amp;\propto \exp \left\{ -\frac{\gamma}{2} \beta^\top \beta \right\} \prod_{i=1}^n \frac{1}{\sqrt{2 \pi \sigma_i^2}} \exp \left\{-\frac{1}{2 \sigma_i^2} (y_i - \beta^\top x_i)^2 \right\} \\
&amp;\propto \exp \left\{ -\frac{1}{2} \beta^\top J \beta + h^\top \beta \right\}
\end{align*}\]</div>
<p>where</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
J &amp;= \gamma I + \sum_{i=1}^n w_i x_i x_i^\top \\
h &amp;= \sum_{i=1}^n w_i y_i x_i \\
w_i &amp;= \frac{1}{\sigma_i^2}.
\end{align*}\]</div>
<p>We can rewrite this as a <strong>multivariate normal distribution</strong> by completing the square,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
p(\beta \mid y, X, \sigma^2)
&amp;\propto \exp \left\{ -\frac{1}{2} (\beta - \mu)^\top \Sigma^{-1} (\beta - \mu) \right\}
\end{align*}\]</div>
<p>where</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mu &amp;= J^{-1} h \\
\Sigma &amp;= J^{-1}
\end{align*}\]</div>
<p>The posterior mean can be written in terms of <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> in compact form,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mu &amp;= (\gamma I + X^\top W X)^{-1} (X^\top W y)
\end{align*}\]</div>
<p>where <span class="math notranslate nohighlight">\(W = \mathrm{diag}((w_1, \ldots, w_n))\)</span>.</p>
<p>We recognize this as the <strong>weighted least squares</strong> estimate with ridge regularization!</p>
</div>
</section>
</section>
<section id="robust-regression-with-students-t-noise">
<h3>Robust Regression with Students’ T Noise<a class="headerlink" href="#robust-regression-with-students-t-noise" title="Link to this heading">#</a></h3>
<p>The model above makes the strong and often unreasonable assumption that we <em>know the noise variance</em> for each data point. A more reasonable assumption is to consider a model with heavy-tailed noise, like the following,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
p(y, \beta \mid X)
&amp;= \mathrm{N}(\beta \mid 0, \gamma^{-1} I) \prod_{i=1}^n \mathrm{St}(y_i \mid \beta^\top x_i, \sigma_0^2, \nu)
\end{align*}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathrm{St}(\mu, \sigma_0^2, \nu)\)</span> denotes the <strong>Students’ t distribution</strong> with mean <span class="math notranslate nohighlight">\(\mu\)</span>, scale <span class="math notranslate nohighlight">\(\sigma_0\)</span>, and <span class="math notranslate nohighlight">\(\nu\)</span> degrees of freedom. Its density is,</p>
<div class="amsmath math notranslate nohighlight" id="equation-f07cb48a-8006-416c-8e22-8ab5c1ffabfb">
<span class="eqno">(3)<a class="headerlink" href="#equation-f07cb48a-8006-416c-8e22-8ab5c1ffabfb" title="Permalink to this equation">#</a></span>\[\begin{align}
    \mathrm{St}(y; \mu, \sigma_0^2, \nu)
    &amp;= \frac{\Gamma(\tfrac{\nu + 1}{2})}{\Gamma(\tfrac{\nu}{2})} \frac{1}{\sqrt{\pi \nu \sigma_0^2}} \left[1 + \frac{\Delta^2}{\nu} \right]^{-\tfrac{\nu + 1}{2}}
\end{align}\]</div>
<p>where <span class="math notranslate nohighlight">\(\Delta^2 = \left(\tfrac{y - \mu}{\sigma_0} \right)^2\)</span> is the squared Mahalanobis distance.
Its mean is <span class="math notranslate nohighlight">\(\mu\)</span> and its variance is <span class="math notranslate nohighlight">\(\frac{\nu}{\nu + 2} \sigma_0^2\)</span>.</p>
<p>Unfortunately, the Student’s t distribution is not conjugate with the Gaussian prior (convince yourself).</p>
</section>
<section id="student-s-t-as-a-scale-mixture-of-gaussians">
<h3>Student’s t as a scale-mixture of Gaussians<a class="headerlink" href="#student-s-t-as-a-scale-mixture-of-gaussians" title="Link to this heading">#</a></h3>
<p>However, here’s a <strong>nifty trick</strong>! The Student’s t density can be written as a scale-mixture of Gaussians,</p>
<div class="amsmath math notranslate nohighlight" id="equation-951e1ca6-00fe-4fb8-8ca7-7db4a7567f77">
<span class="eqno">(4)<a class="headerlink" href="#equation-951e1ca6-00fe-4fb8-8ca7-7db4a7567f77" title="Permalink to this equation">#</a></span>\[\begin{align}
\mathrm{St}\left(y; \mu, \sigma_0^2, \nu \right)
&amp;= \int \mathrm{IGa}(\sigma \mid \tfrac{\nu}{2}, \tfrac{\nu \sigma_0^2}{2}) \, \mathrm{N}(y \mid \mu, \sigma^2) \, \mathrm{d} \sigma^2
\end{align}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathrm{IGa}\)</span> denotes the <strong>inverse gamma distribution</strong>, which has density</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathrm{IGa}(\sigma^2 \mid a, b)
&amp;= \frac{b^a}{\Gamma(a)} (\sigma^2)^{-a - 1} e^{-\frac{b}{\sigma^2}}
\end{align*}\]</div>
<p>When <span class="math notranslate nohighlight">\(a = \nu/2\)</span> and <span class="math notranslate nohighlight">\(b = \nu \sigma_0^2 /2\)</span>, it has mean <span class="math notranslate nohighlight">\(\mathbb{E}[\sigma^2] = \frac{\nu}{\nu - 2} \sigma_0^2\)</span> (for <span class="math notranslate nohighlight">\(\nu &gt; 2\)</span>) and the variance shrinks as <span class="math notranslate nohighlight">\(\nu\)</span> increases.</p>
<p>Notes:</p>
<ul class="simple">
<li><p>The inverse gamma is equivalent to a scaled inverse chi-squared distribution with degrees of freedom <span class="math notranslate nohighlight">\(\nu\)</span> and scale <span class="math notranslate nohighlight">\(\sigma_0^2\)</span>.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(\sigma \sim \mathrm{IGa}(a, b)\)</span> then <span class="math notranslate nohighlight">\(\sigma^{-1} \sim \mathrm{Ga}(a, b)\)</span>, so we can also think of the Student’s t distribution as a continuous mixture of Gaussians with gamma distributed <em>precisions</em>.</p></li>
</ul>
</section>
<section id="augmentation-tricks">
<h3>Augmentation Tricks<a class="headerlink" href="#augmentation-tricks" title="Link to this heading">#</a></h3>
<p>We can use the integral representation of the Student’s t distribution to write our joint distribution as a marginal of an augmented model. To be precise, we can write the joint distribution above as,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
p(y, \beta \mid X)
&amp;= \mathrm{N}(\beta \mid 0, \gamma^{-1} I) \prod_{i=1}^n \mathrm{St}(y_i \mid \beta^\top x_i, \sigma_0^2, \nu) \\
&amp;= \mathrm{N}(\beta \mid 0, \gamma^{-1} I) \prod_{i=1}^n \int \mathrm{IGa}(\sigma_i \mid \tfrac{\nu}{2}, \tfrac{\nu \sigma_0^2}{2}) \, \mathrm{N}(y_i \mid \beta^\top x_i, \sigma_i^2) \, \mathrm{d} \sigma_i^2 \\
&amp;= \int p(y, \beta, \sigma \mid X) \, \mathrm{d}\sigma_1^2 \cdots \mathrm{d} \sigma_n^2
\end{align*}\]</div>
<p>The nice thing about Monte Carlo is that if we can draw samples from the posterior distribution of the augmented model, <span class="math notranslate nohighlight">\(p(\beta, \sigma \mid y, X)\)</span>, then we can simply discard our samples of <span class="math notranslate nohighlight">\(\sigma\)</span> when calculating posterior expectations that are functions of <span class="math notranslate nohighlight">\(\beta\)</span>. That is,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb{E}_{p(\beta \mid y, X)} [g(\beta)]
&amp;= \mathbb{E}_{p(\beta, \sigma^2 \mid y, X)} [g(\beta)] \\
&amp;\approx \frac{1}{M} g(\beta^{(m)}) &amp; \text{where} \; \beta^{(m)}, \sigma^{(m)} &amp;\sim p(\beta, \sigma^2 \mid y, X)
\end{align*}\]</div>
<p>It may sound strictly harder to draw samples from the posterior of <span class="math notranslate nohighlight">\(\beta\)</span> and <span class="math notranslate nohighlight">\(\sigma^2\)</span> than from the posterior of <span class="math notranslate nohighlight">\(\beta\)</span> alone, but that’s not always the case! For example, in this augmented model, note that the conditional distributions have nice closed forms. In particular,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
p(\beta \mid y, X, \sigma^2)
&amp;\propto \mathrm{N}(\beta \mid 0, \gamma^{-1} I) \prod_{i=1}^n \mathrm{N}(y_i \mid \beta^\top x_i, \sigma_i^2).
\end{align*}\]</div>
<p>This is exactly the Bayesian linear regression model with known, heteroskedastic noise that we solved above!</p>
<section id="id2">
<h4>Exercise<a class="headerlink" href="#id2" title="Link to this heading">#</a></h4>
<p>Derive the conditional distribution <span class="math notranslate nohighlight">\(p(\sigma_i^2 \mid y_i, x_i, \beta)\)</span></p>
</section>
<section id="id3">
<h4>Solution<a class="headerlink" href="#id3" title="Link to this heading">#</a></h4>
<div class="dropdown admonition">
<p class="admonition-title">Solution</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
p(\sigma_i^2 \mid y_i, x_i, \beta) &amp;\propto
\mathrm{IGa}(\sigma_i^2 \mid \nu/2, \nu \sigma_0^2 /2) \mathrm{N}(y_i \mid \beta^\top x_i, \sigma_i^2) \\
&amp;\propto (\sigma_i^2)^{-(\nu + 1)/2 - 1} \exp \left\{- \frac{\nu \sigma_0^2 + (y_i - \beta^\top x_i)^2}{2 \sigma_i^2} \right\} \\
&amp;= \mathrm{IGa}(\sigma_i^2 \mid a', b')
\end{align*}\]</div>
<p>where</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
a' &amp;= \frac{\nu + 1}{2} \\
b' &amp;= \frac{\nu \sigma_0^2 + (y_i - \beta^\top x_i)^2}{2}
\end{align*}\]</div>
<p>In other words, the inverse gamma distribution is a conjugate prior for the variance of a normal distribution (with fixed mean).</p>
</div>
</section>
</section>
<section id="id4">
<h3>Generate Synthetic Data<a class="headerlink" href="#id4" title="Link to this heading">#</a></h3>
<p>Now let’s generate some synthetic data from the robust linear regression model. Here, we’ll use just an intercept and a slope, so <span class="math notranslate nohighlight">\(\beta \in \mathbb{R}^2\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">305</span> <span class="o">+</span> <span class="nb">ord</span><span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">))</span>

<span class="c1"># Set constants</span>
<span class="n">num_features</span> <span class="o">=</span> <span class="mi">2</span>                <span class="c1"># first &quot;feature&quot; is all ones</span>
<span class="n">num_data</span> <span class="o">=</span> <span class="mi">20</span>                  <span class="c1"># number of data points to sample</span>
<span class="n">noise_scale</span> <span class="o">=</span> <span class="mf">1.0</span>               <span class="c1"># scale of the Student&#39;s t noise</span>
<span class="n">noise_dof</span> <span class="o">=</span> <span class="mf">2.0</span>                 <span class="c1"># degrees of freedom of the Student&#39;s t noise</span>
<span class="n">prior_scale</span> <span class="o">=</span> <span class="mf">3.</span>                <span class="c1"># std of Gaussian prior \gamma^{-1/2}</span>

<span class="c1"># Fix true weights. Sample features, and responses.</span>
<span class="n">true_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">])</span>

<span class="n">features</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">num_data</span><span class="p">),</span>
    <span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="n">num_data</span><span class="p">,</span> <span class="n">num_features</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
<span class="p">])</span>

<span class="c1"># Sample from a model with a mixture of Gaussians noise distribution</span>
<span class="n">responses</span> <span class="o">=</span> <span class="n">StudentT</span><span class="p">(</span><span class="n">noise_dof</span><span class="p">,</span> <span class="n">features</span> <span class="o">@</span> <span class="n">true_weights</span><span class="p">,</span> <span class="n">noise_scale</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot the data and overlay the OLS estimate of the weights</span>
<span class="k">def</span> <span class="nf">plot_robust_reg_data</span><span class="p">(</span><span class="n">responses</span><span class="p">,</span>
                         <span class="n">features</span><span class="p">,</span>
                         <span class="n">true_weights</span><span class="p">,</span>
                         <span class="n">weights_labels</span><span class="o">=</span><span class="p">(),</span>
                         <span class="n">lim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">features</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">responses</span><span class="p">,</span> <span class="s1">&#39;o&#39;</span><span class="p">)</span>
    <span class="n">xs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">*</span><span class="n">lim</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">true_weights</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">true_weights</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">xs</span><span class="p">,</span> <span class="s1">&#39;-r&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;true&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">weights</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">weights_labels</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">weights</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">xs</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">label</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">lim</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$x$&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$y$&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">ols_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">features</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">features</span><span class="p">,</span> <span class="n">features</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">responses</span><span class="p">)</span>
<span class="n">plot_robust_reg_data</span><span class="p">(</span><span class="n">responses</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">true_weights</span><span class="p">,</span> <span class="p">[(</span><span class="n">ols_weights</span><span class="p">,</span> <span class="s2">&quot;OLS&quot;</span><span class="p">)])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/66f46252d6b3f02dbe72335d11ce9e56fca367533218380cde16b7f92de88a06.png" src="../_images/66f46252d6b3f02dbe72335d11ce9e56fca367533218380cde16b7f92de88a06.png" />
</div>
</div>
</section>
<section id="implement-a-gibbs-sampler">
<h3>Implement a Gibbs sampler<a class="headerlink" href="#implement-a-gibbs-sampler" title="Link to this heading">#</a></h3>
<p>Now implement a Gibbs sampling algorithm to approximate the posterior distribution of the weights in the robust regression model. Specifically, write functions to sample the weights given noise variances and the noise variances given the weights. The Gibbs sampling algorithm will alternate between these two steps.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">log_joint</span><span class="p">(</span><span class="n">responses</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;num_data&quot;</span><span class="p">],</span>
              <span class="n">features</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;num_data num_features&quot;</span><span class="p">],</span>
              <span class="n">weights</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;num_features&quot;</span><span class="p">],</span>
              <span class="n">prior_scale</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
              <span class="n">noise_dof</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
              <span class="n">noise_scale</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Compute the log joint probability of the robust regression model &quot;&quot;&quot;</span>

    <span class="n">lp</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">prior_scale</span><span class="p">)</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="n">lp</span> <span class="o">+=</span> <span class="n">StudentT</span><span class="p">(</span><span class="n">noise_dof</span><span class="p">,</span> <span class="n">features</span> <span class="o">@</span> <span class="n">weights</span><span class="p">,</span> <span class="n">noise_scale</span><span class="p">)</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">responses</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">lp</span>

<span class="c1"># Test that it runs</span>
<span class="n">log_joint</span><span class="p">(</span><span class="n">responses</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">true_weights</span><span class="p">,</span> <span class="n">prior_scale</span><span class="p">,</span> <span class="n">noise_dof</span><span class="p">,</span> <span class="n">noise_scale</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(-40.2235)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">gibbs_step_weights</span><span class="p">(</span><span class="n">responses</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;num_data&quot;</span><span class="p">],</span>
                       <span class="n">features</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;num_data num_features&quot;</span><span class="p">],</span>
                       <span class="n">noise_variances</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;num_data&quot;</span><span class="p">],</span>
                       <span class="n">prior_scale</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> \
                       <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;num_features&quot;</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Perform a Gibbs step to sample the weights from their conditional.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    Next sample of the weights</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">num_features</span> <span class="o">=</span> <span class="n">features</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">J</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">prior_scale</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">num_features</span><span class="p">)</span>
    <span class="n">J</span> <span class="o">+=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;n,nj,nk-&gt;jk&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="n">noise_variances</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">features</span><span class="p">)</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;n,n,nj-&gt;j&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="n">noise_variances</span><span class="p">,</span> <span class="n">responses</span><span class="p">,</span> <span class="n">features</span><span class="p">)</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">J</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">MultivariateNormal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">precision_matrix</span><span class="o">=</span><span class="n">J</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>

<span class="c1"># Test that it runs</span>
<span class="n">dummy_noise_vars</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">num_data</span><span class="p">)</span>
<span class="n">gibbs_step_weights</span><span class="p">(</span><span class="n">responses</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">dummy_noise_vars</span><span class="p">,</span> <span class="n">prior_scale</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([-1.1203,  1.7094])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">gibbs_step_noise_vars</span><span class="p">(</span><span class="n">responses</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;num_data&quot;</span><span class="p">],</span>
                          <span class="n">features</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;num_data num_features&quot;</span><span class="p">],</span>
                          <span class="n">weights</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;num_features&quot;</span><span class="p">],</span>
                          <span class="n">noise_dof</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
                          <span class="n">noise_scale</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> \
                          <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;num_dat&quot;</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Perform a Gibbs step to sample the auxiliary noise variances from their conditionals.</span>

<span class="sd">    Note that the noise variances are conditionally independent and hence can be sampled</span>
<span class="sd">    in parallel.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    Next sample of the noise variances.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">concentration</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">noise_dof</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">preds</span> <span class="o">=</span> <span class="n">features</span> <span class="o">@</span> <span class="n">weights</span>
    <span class="n">rate</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">noise_dof</span> <span class="o">*</span> <span class="n">noise_scale</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">responses</span> <span class="o">-</span> <span class="n">preds</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">Gamma</span><span class="p">(</span><span class="n">concentration</span><span class="p">,</span> <span class="n">rate</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>

<span class="c1"># Test that it runs</span>
<span class="n">gibbs_step_noise_vars</span><span class="p">(</span><span class="n">responses</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">true_weights</span><span class="p">,</span> <span class="n">noise_dof</span><span class="p">,</span> <span class="n">noise_scale</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([ 0.9094,  1.2934, 49.7823,  0.8115,  2.1487,  2.4824,  0.3803, 17.6953,
         0.2016, 11.7235,  4.3357,  0.9562,  0.5754,  0.8171,  0.2200,  1.0080,
         4.2905,  2.2647,  0.6595,  0.8128])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">gibbs</span><span class="p">(</span><span class="n">responses</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;num_data&quot;</span><span class="p">],</span>
          <span class="n">features</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;num_data num_features&quot;</span><span class="p">],</span>
          <span class="n">init_weights</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;num_features&quot;</span><span class="p">],</span>
          <span class="n">prior_scale</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
          <span class="n">noise_dof</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
          <span class="n">noise_scale</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
          <span class="n">num_iters</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">,</span>
          <span class="p">)</span> <span class="o">-&gt;</span> \
         <span class="n">Tuple</span><span class="p">[</span><span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;num_samples num_features&quot;</span><span class="p">],</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;num_samples&quot;</span><span class="p">]]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Run the random walk MH algorithm from the given starting point and for the</span>
<span class="sd">    specified number of iterations.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    Array of weight samples and corresponding log joint probabilities.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Helper function to compute log prob as a function of the weights</span>
    <span class="n">lp</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">weights</span><span class="p">:</span> <span class="n">log_joint</span><span class="p">(</span><span class="n">responses</span><span class="p">,</span>
                                   <span class="n">features</span><span class="p">,</span>
                                   <span class="n">weights</span><span class="p">,</span>
                                   <span class="n">prior_scale</span><span class="p">,</span>
                                   <span class="n">noise_dof</span><span class="p">,</span>
                                   <span class="n">noise_scale</span><span class="p">)</span>

    <span class="c1"># Initialize the loop</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="n">init_weights</span>
    <span class="n">weight_samples</span> <span class="o">=</span> <span class="p">[</span><span class="n">weights</span><span class="p">]</span>
    <span class="n">log_probs</span> <span class="o">=</span> <span class="p">[</span><span class="n">lp</span><span class="p">(</span><span class="n">weights</span><span class="p">)]</span>

    <span class="k">for</span> <span class="n">itr</span> <span class="ow">in</span> <span class="n">progress_bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">num_iters</span><span class="p">)):</span>
        <span class="n">noise_vars</span> <span class="o">=</span> <span class="n">gibbs_step_noise_vars</span><span class="p">(</span><span class="n">responses</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">noise_dof</span><span class="p">,</span> <span class="n">noise_scale</span><span class="p">)</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">gibbs_step_weights</span><span class="p">(</span><span class="n">responses</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">noise_vars</span><span class="p">,</span> <span class="n">prior_scale</span><span class="p">)</span>

        <span class="n">weight_samples</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>
        <span class="n">log_probs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lp</span><span class="p">(</span><span class="n">weights</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">weight_samples</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">log_probs</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="id5">
<h3>Let ‘er rip!<a class="headerlink" href="#id5" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Run Gibbs starting from \beta = 0</span>
<span class="n">init_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_features</span><span class="p">)</span>
<span class="n">weight_samples</span><span class="p">,</span> <span class="n">log_probs</span> <span class="o">=</span> <span class="n">gibbs</span><span class="p">(</span><span class="n">responses</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">init_weights</span><span class="p">,</span> <span class="n">prior_scale</span><span class="p">,</span> <span class="n">noise_dof</span><span class="p">,</span> <span class="n">noise_scale</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">
<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div><div class="output text_html">
    <div>
      <progress value='10000' class='' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>
      100.00% [10000/10000 00:38&lt;00:00]
    </div>
    </div></div>
</div>
</section>
<section id="id6">
<h3>Plot the results<a class="headerlink" href="#id6" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">log_probs</span> <span class="o">/</span> <span class="n">num_data</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">log_joint</span><span class="p">(</span><span class="n">responses</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">true_weights</span><span class="p">,</span> <span class="n">prior_scale</span><span class="p">,</span> <span class="n">noise_dof</span><span class="p">,</span> <span class="n">noise_scale</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_data</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;iteration&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;scaled log joint probability&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0, 0.5, &#39;scaled log joint probability&#39;)
</pre></div>
</div>
<img alt="../_images/30eb505bdc450a6fec76a41cf2acd5b4c1d5e53cfc0413239ef76c98dfd42d9f.png" src="../_images/30eb505bdc450a6fec76a41cf2acd5b4c1d5e53cfc0413239ef76c98dfd42d9f.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">num_features</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">width_ratios</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_features</span><span class="p">):</span>
    <span class="c1"># Plot the trace of the samples</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">weight_samples</span><span class="p">[:,</span> <span class="n">j</span><span class="p">])</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">true_weights</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\beta_</span><span class="si">{}</span><span class="s2">$&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">j</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">num_features</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span> <span class="n">axs</span><span class="p">[</span><span class="n">j</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([])</span>

    <span class="c1"># Plot a histogram of samples</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">weight_samples</span><span class="p">[:,</span> <span class="n">j</span><span class="p">],</span> <span class="mi">20</span><span class="p">,</span> <span class="n">orientation</span><span class="o">=</span><span class="s2">&quot;horizontal&quot;</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">true_weights</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">)</span>

<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;trace plot&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;marginal&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;iteration&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/56ea94fd59aa8d1881983f91779b78278ade8a363a5f5054a170a825726098f6.png" src="../_images/56ea94fd59aa8d1881983f91779b78278ade8a363a5f5054a170a825726098f6.png" />
</div>
</div>
<p>Plot the posterior mean of the weights. Is it any closer to the true weights than the OLS?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">mean_weights</span> <span class="o">=</span> <span class="n">weight_samples</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plot_robust_reg_data</span><span class="p">(</span><span class="n">responses</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">true_weights</span><span class="p">,</span>
                     <span class="p">[(</span><span class="n">ols_weights</span><span class="p">,</span> <span class="s2">&quot;OLS&quot;</span><span class="p">),</span>
                      <span class="p">(</span><span class="n">mean_weights</span><span class="p">,</span> <span class="s2">&quot;Posterior mean&quot;</span><span class="p">)])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/14c741f8986fbb8218ca3e86b17e77605bc4a157e31364076bf4ac0354503612.png" src="../_images/14c741f8986fbb8218ca3e86b17e77605bc4a157e31364076bf4ac0354503612.png" />
</div>
</div>
</section>
</section>
<section id="bayesian-probit-regression">
<h2>Bayesian Probit Regression<a class="headerlink" href="#bayesian-probit-regression" title="Link to this heading">#</a></h2>
<p>There are cool augmentation tricks for Bayesian GLMs too! For example, consider a Bernoulli GLM with the <em>probit</em> link function,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
f(a) &amp;= \int_{-\infty}^a \mathrm{N}(z ; 0, 1) \, \mathrm{d} z \triangleq \Phi(a),
\end{align*}\]</div>
<p>i.e., the normal CDF. Like the logistic function, the normal CDF is a monotonically increasing function that defines a bijective mapping from <span class="math notranslate nohighlight">\(\mathbb{R} \mapsto (0,1)\)</span>. With this mean function, the Bernoulli GLM likelihood is,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
p(y_i \mid x_i, \beta)
&amp;= \mathrm{Bern}(y_i \mid \Phi(\beta^\top x_i)) \\
&amp;= \Phi(\beta^\top x_i)^{y_i} (1 - \Phi(\beta^\top x_i))^{1 - y_i}.
\end{align*}\]</div>
<p><span id="id7">Albert and Chib [<a class="reference internal" href="99_references.html#id5" title="James H Albert and Siddhartha Chib. Bayesian analysis of binary and polychotomous response data. Journal of the American statistical Association, pages 669–679, 1993.">AC93</a>]</span> noted that we can rewrite this likelihood using the integral representation of the mean function, together with a few handy transformations. Namely,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\Phi(a) = \int_{-\infty}^a \mathrm{N}(z ; 0, 1) \, \mathrm{d} z
&amp;= \int_{0}^\infty \mathrm{N}(z; a, 1) \, \mathrm{d} z,
\end{align*}\]</div>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
1 - \Phi(a) &amp;= \int_{-\infty}^0 \mathrm{N}(z; a, 1) \, \mathrm{d} z.
\end{align*}\]</div>
<p>Substituting these expressions into the likelihood above,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
p(y_i \mid x_i, \beta)
&amp;= \mathrm{Bern}(y_i \mid \Phi(\beta^\top x_i)) \\
&amp;= \left[\int_{0}^\infty \mathrm{N}(z_i; \beta^\top x_i, 1) \, \mathrm{d} z_i \right]^{y_i} \left[\int_{-\infty}^0 \mathrm{N}(z_i; \beta^\top x_i, 1) \, \mathrm{d} z_i \right]^{1 - y_i} \\
&amp;= \int_{-\infty}^\infty p(y_i \mid z_i) \, \mathrm{N}(z_i \mid \beta^\top x_i, 1) \, \mathrm{d} z_i
\end{align*}\]</div>
<p>where</p>
<div class="amsmath math notranslate nohighlight" id="equation-0a8829a8-4ea8-4c5b-8792-52ed887147f1">
<span class="eqno">(5)<a class="headerlink" href="#equation-0a8829a8-4ea8-4c5b-8792-52ed887147f1" title="Permalink to this equation">#</a></span>\[\begin{align}
    p(y_i \mid z_i) &amp;=
    \begin{cases}
    \delta_1(y_i) &amp; \text{if } z_i &gt; 0\\
    \delta_0(y_i) &amp; \text{if } z_i \leq 0
    \end{cases}
\end{align}\]</div>
<p>In other words, we can view the Bernoulli likelihood with probit link as the marginal of a joint distribution, <span class="math notranslate nohighlight">\(p(y_i, z_i \mid x_i, \beta)\)</span>. In this particular joint distribution, <span class="math notranslate nohighlight">\(y_i\)</span> is deterministic given <span class="math notranslate nohighlight">\(z_i\)</span>.</p>
<p>Another way to think about this augmented model is that we only observe the sign  of a latent variable <span class="math notranslate nohighlight">\(z_i\)</span> (where <span class="math notranslate nohighlight">\(y_i=0\)</span> if <span class="math notranslate nohighlight">\(z_i\)</span> is negative, and <span class="math notranslate nohighlight">\(y_i=1\)</span> if it is positive). The latent variable is itself a noisy perturbation of the linear model’s prediction, <span class="math notranslate nohighlight">\(\beta^\top x_i\)</span>. The challenge is, given these observations of the sign, to estimate the underlying weights <span class="math notranslate nohighlight">\(\beta\)</span>.</p>
<section id="id8">
<h3>Exercise<a class="headerlink" href="#id8" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p>Show that the conditional distribution of the latent variable <span class="math notranslate nohighlight">\(z_i\)</span> in the augmented model is a <strong>truncated normal distribution</strong>.</p></li>
<li><p>Show that the conditional distribution of the weights given the latent variables <span class="math notranslate nohighlight">\(z_i\)</span> is multivariate normal.</p></li>
<li><p>Implement a Gibbs sampling algorithm to draw samples from the joint distribution <span class="math notranslate nohighlight">\(p(z, \beta \mid y, X)\)</span>.</p></li>
</ol>
</section>
</section>
<section id="augmentation-schemes-for-bayesian-logistic-regression">
<h2>Augmentation Schemes for Bayesian Logistic Regression<a class="headerlink" href="#augmentation-schemes-for-bayesian-logistic-regression" title="Link to this heading">#</a></h2>
<p>We can apply the same logic to the logistic regression case by viewing the logistic function as the CDF of a distribution on the real line. It turns out the corresponding distribution is the aptly named <a class="reference external" href="https://en.wikipedia.org/wiki/Logistic_distribution"><strong>logistic distribution</strong></a> with pdf,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathrm{Logistic}(u; 0, 1) = \frac{\mathrm{d}}{\mathrm{d} u} \sigma(u) = \sigma(u) ( 1 - \sigma(u)) = \frac{e^u}{(1+e^u)^2}.
\end{align*}\]</div>
<p>Then, we can interpret the Bernoulli likelihood with logistic mean function as a marginal of the auxiliary model,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
u_i &amp;\sim \mathrm{Logistic}(0, 1) \\
z_i &amp;= x_i^\top \beta + u_i \\
y_i &amp;= \mathbb{I}[z_i &gt; 0].
\end{align*}\]</div>
<p>Or, as with the probit model above,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
z_i &amp;\sim \mathrm{Logistic}(x_i^\top \beta, 1) \\
y_i &amp;= \mathbb{I}[z_i &gt; 0].
\end{align*}\]</div>
<p>The logistic distribution is heavier tailed than a Gaussian. In fact, its is closer to a Student’s t distribution with 9 degrees of freedom and slightly inflated scale, as shown below</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">zs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">zs</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">zs</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="o">-</span><span class="n">zs</span><span class="p">),</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;logistic&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">zs</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">StudentT</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">1.55</span><span class="p">)</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">zs</span><span class="p">)),</span> <span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;St(0, 1.55, 9)&quot;</span><span class="p">,</span> <span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">zs</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">zs</span><span class="p">)),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;normal&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$z$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$p(z)$&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0, 0.5, &#39;$p(z)$&#39;)
</pre></div>
</div>
<img alt="../_images/c23c46b5d3cab9d242785ab6235e7fec46a6a27cec5339071c298c7b3f80674f.png" src="../_images/c23c46b5d3cab9d242785ab6235e7fec46a6a27cec5339071c298c7b3f80674f.png" />
</div>
</div>
<p>This motivates a simple approximation of <span class="math notranslate nohighlight">\(z_i \sim \mathrm{St}(9, x_i^\top \beta, 1.55)\)</span>. Using this approximation, you can perform Gibbs sampling on the conjugate model, alternating between updating <span class="math notranslate nohighlight">\(\beta\)</span> from its Gaussian conditional, and updating <span class="math notranslate nohighlight">\(z_i\)</span> from its truncated Student’s t conditional.</p>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading">#</a></h2>
<p>This lecture looks works through some actual implementations of MCMC algorithms and develops some useful tricks for Gibbs sampling via augmentation. There are some even better approaches out there for Gibbs sampling in logistic models, and we’ll explore them in the homework assignment!</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./lectures"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="06_bayes.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Bayesian Inference</p>
      </div>
    </a>
    <a class="right-next"
       href="08_sparse_glms.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Sparse GLMs</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#setup">Setup</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-logistic-regression">Bayesian Logistic Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model">Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#generate-synthetic-data">Generate Synthetic Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#log-probability">Log Probability</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#metropolis-hastings">Metropolis-Hastings</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#implement-a-single-rwmh-step">Implement a single RWMH step</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#implement-the-mcmc-loop">Implement the MCMC loop</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#let-er-rip">Let ‘er rip!</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#plot-the-results">Plot the results</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise">Exercise</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#robust-bayesian-linear-regression">Robust Bayesian Linear Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#primer-bayesian-linear-regression-with-heteroskedastic-noise">Primer: Bayesian Linear Regression with Heteroskedastic Noise</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Exercise</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#solution">Solution</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#robust-regression-with-students-t-noise">Robust Regression with Students’ T Noise</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#student-s-t-as-a-scale-mixture-of-gaussians">Student’s t as a scale-mixture of Gaussians</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#augmentation-tricks">Augmentation Tricks</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Exercise</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Solution</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Generate Synthetic Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implement-a-gibbs-sampler">Implement a Gibbs sampler</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Let ‘er rip!</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Plot the results</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-probit-regression">Bayesian Probit Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">Exercise</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#augmentation-schemes-for-bayesian-logistic-regression">Augmentation Schemes for Bayesian Logistic Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Scott Linderman
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>