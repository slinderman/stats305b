
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Transformers &#8212; STATS 305B: Models and Algorithms for Discrete Data</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="../_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=888ff710"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"mba": "\\boldsymbol{a}", "mbb": "\\boldsymbol{b}", "mbc": "\\boldsymbol{c}", "mbd": "\\boldsymbol{d}", "mbe": "\\boldsymbol{e}", "mbf": "\\boldsymbol{f}", "mbg": "\\boldsymbol{g}", "mbh": "\\boldsymbol{h}", "mbi": "\\boldsymbol{i}", "mbj": "\\boldsymbol{j}", "mbk": "\\boldsymbol{k}", "mbl": "\\boldsymbol{l}", "mbm": "\\boldsymbol{m}", "mbn": "\\boldsymbol{n}", "mbo": "\\boldsymbol{o}", "mbp": "\\boldsymbol{p}", "mbq": "\\boldsymbol{q}", "mbr": "\\boldsymbol{r}", "mbs": "\\boldsymbol{s}", "mbt": "\\boldsymbol{t}", "mbu": "\\boldsymbol{u}", "mbv": "\\boldsymbol{v}", "mbw": "\\boldsymbol{w}", "mbx": "\\boldsymbol{x}", "mby": "\\boldsymbol{y}", "mbz": "\\boldsymbol{z}", "mbA": "\\boldsymbol{A}", "mbB": "\\boldsymbol{B}", "mbC": "\\boldsymbol{C}", "mbD": "\\boldsymbol{D}", "mbE": "\\boldsymbol{E}", "mbF": "\\boldsymbol{F}", "mbG": "\\boldsymbol{G}", "mbH": "\\boldsymbol{H}", "mbI": "\\boldsymbol{I}", "mbJ": "\\boldsymbol{J}", "mbK": "\\boldsymbol{K}", "mbL": "\\boldsymbol{L}", "mbM": "\\boldsymbol{M}", "mbN": "\\boldsymbol{N}", "mbO": "\\boldsymbol{O}", "mbP": "\\boldsymbol{P}", "mbQ": "\\boldsymbol{Q}", "mbR": "\\boldsymbol{R}", "mbS": "\\boldsymbol{S}", "mbT": "\\boldsymbol{T}", "mbU": "\\boldsymbol{U}", "mbV": "\\boldsymbol{V}", "mbW": "\\boldsymbol{W}", "mbX": "\\boldsymbol{X}", "mbY": "\\boldsymbol{Y}", "mbZ": "\\boldsymbol{Z}", "bbA": "\\mathbb{A}", "bbB": "\\mathbb{B}", "bbC": "\\mathbb{C}", "bbD": "\\mathbb{D}", "bbE": "\\mathbb{E}", "bbG": "\\mathbb{G}", "bbH": "\\mathbb{H}", "bbI": "\\mathbb{I}", "bbJ": "\\mathbb{J}", "bbK": "\\mathbb{K}", "bbL": "\\mathbb{L}", "bbM": "\\mathbb{M}", "bbN": "\\mathbb{N}", "bbO": "\\mathbb{O}", "bbP": "\\mathbb{P}", "bbQ": "\\mathbb{Q}", "bbR": "\\mathbb{R}", "bbS": "\\mathbb{S}", "bbT": "\\mathbb{T}", "bbU": "\\mathbb{U}", "bbV": "\\mathbb{V}", "bbW": "\\mathbb{W}", "bbX": "\\mathbb{X}", "bbY": "\\mathbb{Y}", "bbZ": "\\mathbb{Z}", "cA": "\\mathcal{A}", "cB": "\\mathcal{B}", "cC": "\\mathcal{C}", "cD": "\\mathcal{D}", "cE": "\\mathcal{E}", "cG": "\\mathcal{G}", "cH": "\\mathcal{H}", "cI": "\\mathcal{I}", "cJ": "\\mathcal{J}", "cK": "\\mathcal{K}", "cL": "\\mathcal{L}", "cM": "\\mathcal{M}", "cN": "\\mathcal{N}", "cO": "\\mathcal{O}", "cP": "\\mathcal{P}", "cQ": "\\mathcal{Q}", "cR": "\\mathcal{R}", "cS": "\\mathcal{S}", "cT": "\\mathcal{T}", "cU": "\\mathcal{U}", "cV": "\\mathcal{V}", "cW": "\\mathcal{W}", "cX": "\\mathcal{X}", "cY": "\\mathcal{Y}", "cZ": "\\mathcal{Z}", "mbalpha": "\\boldsymbol{\\alpha}", "mbbeta": "\\boldsymbol{\\beta}", "mbgamma": "\\boldsymbol{\\gamma}", "mbdelta": "\\boldsymbol{\\delta}", "mbepsilon": "\\boldsymbol{\\epsilon}", "mbchi": "\\boldsymbol{\\chi}", "mbeta": "\\boldsymbol{\\eta}", "mbiota": "\\boldsymbol{\\iota}", "mbkappa": "\\boldsymbol{\\kappa}", "mblambda": "\\boldsymbol{\\lambda}", "mbmu": "\\boldsymbol{\\mu}", "mbnu": "\\boldsymbol{\\nu}", "mbomega": "\\boldsymbol{\\omega}", "mbtheta": "\\boldsymbol{\\theta}", "mbphi": "\\boldsymbol{\\phi}", "mbpi": "\\boldsymbol{\\pi}", "mbpsi": "\\boldsymbol{\\psi}", "mbrho": "\\boldsymbol{\\rho}", "mbsigma": "\\boldsymbol{\\sigma}", "mbtau": "\\boldsymbol{\\tau}", "mbupsilon": "\\boldsymbol{\\upsilon}", "mbxi": "\\boldsymbol{\\xi}", "mbzeta": "\\boldsymbol{\\zeta}", "mbvarepsilon": "\\boldsymbol{\\varepsilon}", "mbvarphi": "\\boldsymbol{\\varphi}", "mbvartheta": "\\boldsymbol{\\vartheta}", "mbvarrho": "\\boldsymbol{\\varrho}", "mbDelta": "\\boldsymbol{\\Delta}", "mbGamma": "\\boldsymbol{\\Gamma}", "mbLambda": "\\boldsymbol{\\Lambda}", "mbOmega": "\\boldsymbol{\\Omega}", "mbPhi": "\\boldsymbol{\\Phi}", "mbPsi": "\\boldsymbol{\\Psi}", "mbPi": "\\boldsymbol{\\Pi}", "mbSigma": "\\boldsymbol{\\Sigma}", "mbTheta": "\\boldsymbol{\\Theta}", "mbUpsilon": "\\boldsymbol{\\Upsilon}", "mbXi": "\\boldsymbol{\\Xi}", "mbzero": "\\boldsymbol{0}", "mbone": "\\boldsymbol{1}", "iid": ["\\stackrel{\\text{iid}}{#1}", 1], "ind": ["\\stackrel{\\text{ind}}{#1}", 1], "dif": "\\mathop{}\\!\\mathrm{d}", "diag": "\\textrm{diag}", "supp": "\\textrm{supp}", "Tr": "\\textrm{Tr}", "E": "\\mathbb{E}", "Var": "\\textrm{Var}", "Cov": "\\textrm{Cov}", "reals": "\\mathbb{R}", "naturals": "\\mathbb{N}", "KL": ["D_{\\textrm{KL}}\\left(#1\\;\\|\\;#2\\right)", 2]}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lectures/13_transformers';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Random Graphs Models" href="15_graphs.html" />
    <link rel="prev" title="Recurrent Neural Networks" href="12_rnns.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
  
    <p class="title logo__title">STATS 305B: Models and Algorithms for Discrete Data</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Overview
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Notes</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01_distributions.html">Discrete Distributions and the Basics of Statistical Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_contingency_tables.html">Contingency Tables</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_logreg.html">Logistic Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="04_expfam.html">Exponential Families</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_glms.html">Generalized Linear Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="06_bayes.html">Bayesian Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_bayes_glms_soln.html">Bayesian GLMs</a></li>
<li class="toctree-l1"><a class="reference internal" href="08_sparse_glms.html">Sparse GLMs</a></li>
<li class="toctree-l1"><a class="reference internal" href="09_mixtures.html">Mixture Models and EM</a></li>
<li class="toctree-l1"><a class="reference internal" href="10_hmms.html">Hidden Markov Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="11_vaes.html">Variational Autoencoders</a></li>
<li class="toctree-l1"><a class="reference internal" href="11_vaes_demo.html">Demo: Neural Networks and VAEs</a></li>
<li class="toctree-l1"><a class="reference internal" href="12_rnns.html">Recurrent Neural Networks</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="15_graphs.html">Random Graphs Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="16_diffusion.html">Denoising Diffusion Models</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Assignments</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../assignments/hw0/hw0.html">HW0: PyTorch Primer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../assignments/hw1/hw1.html">HW1: Logistic Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../assignments/hw2/hw2.html">HW2: Bayesian GLMs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../assignments/hw3/hw3.html">HW3: Hidden Markov Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../assignments/hw4/hw4.html">HW4: Large Language Models</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">References</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="99_references.html">References</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/lectures/13_transformers.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Transformers</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#preliminaries">Preliminaries</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#attention">Attention</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#self-attention">Self-Attention</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-headed-self-attention">Multi-Headed Self-Attention</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#token-wise-nonlinearity">Token-wise Nonlinearity</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#residual-connections">Residual Connections</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-norm">Layer Norm</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#positional-encodings">Positional Encodings</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#autoregressive-modeling">Autoregressive Modeling</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training">Training</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="transformers">
<h1>Transformers<a class="headerlink" href="#transformers" title="Link to this heading">#</a></h1>
<p>RNNs are natural models for sequential data, but the <span class="math notranslate nohighlight">\(\cO(T)\)</span> time complexity of evaluation and backpropagating gradients is a severe limitation. In modern machine learning, one of the deciding factors is how many training epochs you can perform for a fixed computational budget. To that end, architectures that can process an entire sequence in parallel are advantageous. Transformers are one such architecture.</p>
<p>Transformers underlie large language models (LLMs) like Open AI’s ChatGPT and Google’s Gemini. They are also widely used in computer vision and other domains of machine learning. This lecture will walk through the basic building blocks of a transformer: self-attention, token-wise nonlinear transformations, layer norm, and positional encodings. We will focus on modeling sequential data. We will follow the presentation of <span id="id1">Turner [<a class="reference internal" href="99_references.html#id18" title="Richard E Turner. An introduction to transformers. arXiv preprint arXiv:2304.10557, 2023.">Tur23</a>]</span>, but we will make some slight modifications to the notation to be consistent with our previous notes and Homework 4.</p>
<section id="preliminaries">
<h2>Preliminaries<a class="headerlink" href="#preliminaries" title="Link to this heading">#</a></h2>
<p>Let <span class="math notranslate nohighlight">\(\mbX^{(0)} \in \reals^{T \times D}\)</span> denote our data matrix with row <span class="math notranslate nohighlight">\(\mbx_t^{(0)} \in \reals^D\)</span> representing the <span class="math notranslate nohighlight">\(t\)</span>-th <strong>token</strong> in the sequence. For example, the tokens could be a vector embedding of a word, sub-word, or character. The embeddings may be fixed or learned as part of the model.</p>
<p>The output of the transformer will be another matrix of the same shape, <span class="math notranslate nohighlight">\(\mbX^{(M)} \in \reals^{T \times D}\)</span>. These output features can be used for downstream tasks like sentiment classification, machine translation, or autoregressive modeling.</p>
<p>The output results from a stack of transformer blocks,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mbX^{(m)} &amp;= \texttt{transformer-block}(\mbX^{(m-1)})
\end{align*}\]</div>
<p>Each block consists of two stages: one that operates vertically, combining information across the sequence length; another that operates horizontally, combining information across feature dimensions.</p>
</section>
<section id="attention">
<h2>Attention<a class="headerlink" href="#attention" title="Link to this heading">#</a></h2>
<p>The first stage combines information across sequence length using a mechanism called <strong>attention</strong>. Mathematically, attention is just a weighted average,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mbY^{(m)} &amp;= \mbA^{(m)} \mbX^{(m-1)},
\end{align*}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mbA^{(m)} \in \reals_+^{T \times T}\)</span> is a row-stochastic attention matrix. That is, <span class="math notranslate nohighlight">\(\sum_{s} A_{ts}^{(m)} = 1\)</span> for all <span class="math notranslate nohighlight">\(t\)</span>. Intuitively, <span class="math notranslate nohighlight">\(A_{t,s}^{(m)}\)</span> indicates how much output location <span class="math notranslate nohighlight">\(t\)</span> attends to input location <span class="math notranslate nohighlight">\(s\)</span>.</p>
<p>When we are using transformers for autoregressive sequence modeling, we constrain the attention matrix to be <strong>causal</strong> by requiring <span class="math notranslate nohighlight">\(A_{t,s}^{(m)} = 0\)</span> for all <span class="math notranslate nohighlight">\(t &lt; s\)</span>. In other words, the matrix is <strong>lower triangular</strong>.</p>
<section id="self-attention">
<h3>Self-Attention<a class="headerlink" href="#self-attention" title="Link to this heading">#</a></h3>
<p>Where does the attention matrix come from? In a transformer, the attention weights are determined by the pairwise similarity of tokens in the sequence. The simplest instantiation of this idea would be something like,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
A_{t,s} &amp;\propto \exp \left\{ \mbx_t^\top \mbx_s \right\}
\end{align*}\]</div>
<p>Once normalized,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
A_{t,s} &amp;= \frac{\exp \{ \mbx_t^\top \mbx_s\}}{\sum_{s'=1}^T \exp\{\mbx_t^\top \mbx_{s'}\}}.
\end{align*}\]</div>
<p>(Note: we have dropped the superscript <span class="math notranslate nohighlight">\({}^{(m)}\)</span> for clarity in this section.)</p>
<p>This approach implies that attention depends equally on all <span class="math notranslate nohighlight">\(D\)</span> dimensions of the embedding. In practice, some of those dimensions may convey different kinds of information that may be of greater or lesser relevance for the attention mechanism. One way to allow for this is to project the tokens into a feature space before computing the attention weights,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
A_{t,s} &amp;= \frac{\exp \{ (\mbU \mbx_t)^\top (\mbU \mbx_s)\}}{\sum_{s'=1}^T \exp\{(\mbU \mbx_t)^\top (\mbU \mbx_{s'}) \}}
\end{align*}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mbU \in \reals^{K \times D}\)</span> with <span class="math notranslate nohighlight">\(K &lt; D\)</span>.</p>
<p>Still, the numerator in this attention weight is symmetric. If we think of the attention weight as specifying how relevant token <span class="math notranslate nohighlight">\(\mbx_s\)</span> is to updating the feature representation for token <span class="math notranslate nohighlight">\(\mbx_t\)</span>, then directionality may matter. Personifying the tokens a bit, <span class="math notranslate nohighlight">\(\mbx_t\)</span> may need to know a certain kind of information to update its representation, and <span class="math notranslate nohighlight">\(\mbx_s\)</span> may be able to provide that information, but the converse need not be true.</p>
<p>Transformers use a more general form of attention to address this asymmetry,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
A_{t,s} &amp;= \frac{\exp \{ (\mbU_q \mbx_t)^\top (\mbU_k \mbx_s)\}}{\sum_{s'=1}^T \exp\{(\mbU_q \mbx_t)^\top (\mbU_k \mbx_{s'}) \}},
\end{align*}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mbU_q \mbx_t \in \reals^{K}\)</span> are the <strong>queries</strong> and <span class="math notranslate nohighlight">\(\mbU_k \mbx_s \in \reals^{K}\)</span> are the <strong>keys</strong>.</p>
<p>The parameters <span class="math notranslate nohighlight">\(\mbU_q \in \reals^{K \times D}\)</span> and <span class="math notranslate nohighlight">\(\mbU_k \in \reals^{K \times D}\)</span> are the two parameters defining the self-attention mechanism.</p>
<div class="admonition-causal-attention admonition">
<p class="admonition-title">Causal attention</p>
<p>To enforce causality in the attention layer, we simply zero out the upper triangular part of the attention matrix and normalize the rows appropriately,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
A_{t,s} &amp;= \frac{\exp \{ (\mbU_q \mbx_t)^\top (\mbU_k \mbx_s)\}}{\textcolor{red}{\sum_{s'=1}^{t}} \exp\{(\mbU_q \mbx_t)^\top (\mbU_k \mbx_{s'})\}} \cdot \bbI[t \geq s].
\end{align*}\]</div>
</div>
<div class="admonition-comparison-to-rnns admonition">
<p class="admonition-title">Comparison to RNNs</p>
<p>Compare the self-attention mechanism to the information processing in an RNN. Rather than propagating information about past tokens via a hidden state, a transformer with causal attention can directly attend to any one of the past tokens.</p>
</div>
<div class="admonition-connection-to-convolutional-neural-networks-cnns admonition">
<p class="admonition-title">Connection to Convolutional Neural Networks (CNNs)</p>
<p>If the attention weights were only a function of the distance between tokens, <span class="math notranslate nohighlight">\(A_{t,s} = a_{t-s}\)</span>, then the matrix <span class="math notranslate nohighlight">\(\mbA\)</span> would be a <strong>Toeplitz matrix</strong>. Multiplication by a Toeplitz matrix corresponds to a discrete convolution. From this perspective, we can think of the attention mechanism in a transformer as a generalization of convolution that allows for input-dependent and time-varying filters.</p>
</div>
</section>
<section id="multi-headed-self-attention">
<h3>Multi-Headed Self-Attention<a class="headerlink" href="#multi-headed-self-attention" title="Link to this heading">#</a></h3>
<p>Just as in a CNN each layer performs convolutions with a bank of filters in parallel, in a transformer each block uses a bank of <span class="math notranslate nohighlight">\(H\)</span> <strong>attention heads</strong> in parallel.  Let,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mbY^{(m,h)} &amp;= \mbA^{(m,h)} \mbX^{(m-1)} \in \reals^{T \times D}
\end{align*}\]</div>
<p>where</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
A_{t,s}^{(m,h)} &amp;= 
\frac{\exp \{ (\mbU_q^{(m,h)} \mbx_t^{(m-1)})^\top (\mbU_k^{(m,h)} \mbx_s^{(m-1)})\}}{\sum_{s'=1}^T \exp\{(\mbU_q^{(m,h)} \mbx_t^{(m-1)})^\top (\mbU_k^{(m,h)} \mbx_{s'}^{(m-1)}) \}},
\end{align*}\]</div>
<p>is an attention weight at layer <span class="math notranslate nohighlight">\(m\)</span> and head <span class="math notranslate nohighlight">\(h\)</span> for <span class="math notranslate nohighlight">\(h=1,\ldots,H\)</span>. (Now you see why we dropped superscripts above — the notation is a handful!)</p>
<p>The outputs of the attention heads are either concatenated or linearly combined,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mbY^{(m)} &amp;= \sum_{h=1}^H \mbY^{(m,h)} (\mbV^{(m,h)})^\top \\
&amp;\triangleq \texttt{mhsa}(\mbX^{(m-1)}).
\end{align*}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mbV^{(m,h)} \in \reals^{D \times D}\)</span> is a read-out matrix for layer <span class="math notranslate nohighlight">\(m\)</span>, head <span class="math notranslate nohighlight">\(h\)</span>.
We denote the multi-headed self-attention (MHSA) mapping by <span class="math notranslate nohighlight">\(\texttt{mhsa}(\cdot)\)</span>.</p>
<div class="admonition-queries-keys-and-values admonition">
<p class="admonition-title">Queries, Keys, and Values</p>
<p>The original transformer paper presents the output of a single head as a set of values,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mbY^{(m,h)} &amp;= \mbA^{(m,h)} (\mbX^{(m-1)} (\mbU_v^{(m,h)})^\top) \in \reals^{T \times K}
\end{align*}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mbU_v^{(m,h)} \in \reals^{K \times D}\)</span> is a <strong>value matrix</strong>. Then the attention mechanism can be though of as taking a weighted average of <strong>values</strong> <span class="math notranslate nohighlight">\(\mbU_v \mbx_t\)</span> where the weights are determined by an inner product between the queries and keys.</p>
<p>The final output is projected back into the original token dimension and linearly combined,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mbY^{(m)} &amp;= \sum_{h=1}^H \mbY^{(m,h)} (\mbU_o^{(m,h)})^\top 
\end{align*}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mbU_o^{(m,h)} \in \reals^{D \times K}\)</span> is an <strong>output matrix</strong> that maps from values to new tokens.</p>
<p>This formulation corresponds to a low-rank read-out matrix <span class="math notranslate nohighlight">\(\mbV = \mbU_o \mbU_v^\top\)</span>, where we have again dropped the superscripts for clarity.</p>
</div>
</section>
</section>
<section id="token-wise-nonlinearity">
<h2>Token-wise Nonlinearity<a class="headerlink" href="#token-wise-nonlinearity" title="Link to this heading">#</a></h2>
<p>After applying the multi-headed self-attention to obtain <span class="math notranslate nohighlight">\(\mbY^{(m)}\)</span>, the transformer applies a token-wise nonlinear transformation to nonlinearly mix the feature dimensions. This is done with a simple feedforward neural network, also known as a <strong>multilayer perceptron (MLP)</strong>,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mbx_t^{(m)} &amp;= \texttt{mlp}(\mby_t^{(m)})
\end{align*}\]</div>
<p>Note that the same function is applied to all positions <span class="math notranslate nohighlight">\(t\)</span>.</p>
<div class="warning admonition">
<p class="admonition-title">Computational Complexity</p>
<p>The MLP typically has hidden dimensions of at least <span class="math notranslate nohighlight">\(D\)</span>, so the computational complexity of this step is <span class="math notranslate nohighlight">\(\cO(TD^2)\)</span>. For transformers with very large featured dimensions, this can be the dominant cost.</p>
</div>
</section>
<section id="residual-connections">
<h2>Residual Connections<a class="headerlink" href="#residual-connections" title="Link to this heading">#</a></h2>
<p>Rather than parameterizing <span class="math notranslate nohighlight">\(\mbX^{(m)}\)</span> as the output of the MLP, a common trick in deep learning is to use residual connections. The idea is simple: when the input and output are of the same dimensionality, we can let the network learn the <strong>residual</strong>, which is typically smaller in magnitude than the overall funciton.</p>
<p>Transformers use residual connections for both the multi-headed self-attention step and the MLP. So,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mbY^{(m)} &amp;= \mbX^{(m-1)} + \texttt{mhsa}(\mbX^{(m-1)}) \\
\mbX^{(m)} &amp;= \mbY^{(m)} + \texttt{mlp}(\mbY^{(m)})
\end{align*}\]</div>
</section>
<section id="layer-norm">
<h2>Layer Norm<a class="headerlink" href="#layer-norm" title="Link to this heading">#</a></h2>
<p>Finally, it is important to use other deep learning “tricks” like LayerNorm to stabilize training. In a transformer, LayerNorm amounts to z-scoring each token <span class="math notranslate nohighlight">\(\mbx_t\)</span> and then applying a learned shift and scale to each feature dimension,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\texttt{layer-norm}(\mbx_t) 
&amp;= \mbbeta + \mbgamma \odot \left( \frac{\mbx_t - \texttt{mean}(\mbx_t)}{\texttt{std}(\mbx_t)} \right)
\end{align*}\]</div>
<p>where</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\texttt{mean}(\mbx_t) &amp;= \frac{1}{D} \sum_{d=1}^D x_{t,d} \\
\texttt{std}(\mbx_t) &amp;= \Big(\frac{1}{D} \sum_{d=1}^D (x_{t,d} - \texttt{mean}(\mbx_t))^2 \Big)^{\frac{1}{2}}
\end{align*}\]</div>
<p>and <span class="math notranslate nohighlight">\(\mbbeta, \mbgamma \in \reals^D\)</span> are learned parameters.</p>
<p>LayerNorm is typically applied before the multi-headed self-attention and MLP steps,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\overline{\mbX}^{(m-1)} &amp;= \texttt{layer-norm}(\mbX^{(m-1)}) \\
\mbY^{(m)} &amp;= \overline{\mbX}^{(m-1)} + \texttt{mhsa}(\overline{\mbX}^{(m-1)}) \\
\overline{\mbY}^{(m)} &amp;= \texttt{layer-norm}(\mbY^{(m)}) \\
\mbX^{(m)} &amp;= \overline{\mbY}^{(m)} + \texttt{mlp}(\overline{\mbY}^{(m)})
\end{align*}\]</div>
<p>This defines one <span class="math notranslate nohighlight">\(\texttt{transformer-block}\)</span>.</p>
<p>A transformer stacks <span class="math notranslate nohighlight">\(M\)</span> <span class="math notranslate nohighlight">\(\texttt{transformer-blocks}\)</span> on top of one another to produce a deep sequence-to-sequence model.</p>
</section>
<section id="positional-encodings">
<h2>Positional Encodings<a class="headerlink" href="#positional-encodings" title="Link to this heading">#</a></h2>
<p>Except for the lower triangular constraint on the attention matrices, the transformer architecture knows nothing about the relative positions of the tokens. Absent this constraint, the transformer essentially treats the data as an <strong>unordered set</strong> of tokens. This can actually be a feature! It allows the transformer to act on a wide range of datasets aside from just sequences. For example, transformers are often applied to images by chunking the image up into patches and embedding each one.</p>
<p>However, when the data posses some spatial or temporal structure, it is helpful to include that information in the embedding. A simple way to do so is to add position and content in the token,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mbx_t^{(0)} &amp;= \mbc_t + \mbp_t,
\end{align*}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mbc_t \in \reals^D\)</span> is an embedding of the content and <span class="math notranslate nohighlight">\(\mbp_t \in \reals^D\)</span> encodes the position (e.g., with a set of sinusoidal basis functions).</p>
</section>
<section id="autoregressive-modeling">
<h2>Autoregressive Modeling<a class="headerlink" href="#autoregressive-modeling" title="Link to this heading">#</a></h2>
<p>To use a transformer for autoregressive modeling, we need to make predictions from the final layer’s representations. If the goal is to predict the next word label <span class="math notranslate nohighlight">\(\ell_{t+1} \in \{1,\ldots,V\}\)</span> based on encodings of past words <span class="math notranslate nohighlight">\(\mbx_{1:t}^{(0)}\)</span>, where <span class="math notranslate nohighlight">\(V\)</span> is the vocabulary size, then we could use a categorical distribution,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\ell_{t+1} &amp;\sim \mathrm{Cat}(\mathrm{softmax}(\mbW \mbx_t^{(M)}))
\end{align*}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mbW \in \reals^{V \times D}\)</span>. Like the hidden states in an RNN, the final layer’s representations <span class="math notranslate nohighlight">\(\mbx_t^{(M)}\)</span> combine information from all tokens up to and including index <span class="math notranslate nohighlight">\(t\)</span>.</p>
</section>
<section id="training">
<h2>Training<a class="headerlink" href="#training" title="Link to this heading">#</a></h2>
<p>Training deep neural networks is somewhat of a dark art. Standard practice is to use the Adam optimizer with a bag of tricks including gradient clipping, learning rate annealing schedules, increasing minibatch sizes, dropout, etc. Generally, treat these algorithmic decisions as hyperparameters to be tuned. We won’t try to put any details in writing lest you overfit to them.</p>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading">#</a></h2>
<p>Transformers are a workhorse of modern machine learning and key to many of the impressive advances over recent years. However, there are still areas for improvement. For example, the computational cost of attention is <span class="math notranslate nohighlight">\(\cO(T^2)\)</span>, and a lot of work has gone into cutting that down. Likewise, while the transformer allows for predictions to be made in parallel across an entire sequence, sampling from the learned autoregressive model is still takes linear time. In the lectures ahead, we’ll discuss other recent architectures that address some of these concerns.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./lectures"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="12_rnns.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Recurrent Neural Networks</p>
      </div>
    </a>
    <a class="right-next"
       href="15_graphs.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Random Graphs Models</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#preliminaries">Preliminaries</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#attention">Attention</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#self-attention">Self-Attention</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-headed-self-attention">Multi-Headed Self-Attention</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#token-wise-nonlinearity">Token-wise Nonlinearity</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#residual-connections">Residual Connections</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-norm">Layer Norm</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#positional-encodings">Positional Encodings</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#autoregressive-modeling">Autoregressive Modeling</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training">Training</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Scott Linderman
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>