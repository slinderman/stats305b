
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Hidden Markov Models &#8212; STATS 305B: Models and Algorithms for Discrete Data</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="../_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=888ff710"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"mba": "\\boldsymbol{a}", "mbb": "\\boldsymbol{b}", "mbc": "\\boldsymbol{c}", "mbd": "\\boldsymbol{d}", "mbe": "\\boldsymbol{e}", "mbf": "\\boldsymbol{f}", "mbg": "\\boldsymbol{g}", "mbh": "\\boldsymbol{h}", "mbi": "\\boldsymbol{i}", "mbj": "\\boldsymbol{j}", "mbk": "\\boldsymbol{k}", "mbl": "\\boldsymbol{l}", "mbm": "\\boldsymbol{m}", "mbn": "\\boldsymbol{n}", "mbo": "\\boldsymbol{o}", "mbp": "\\boldsymbol{p}", "mbq": "\\boldsymbol{q}", "mbr": "\\boldsymbol{r}", "mbs": "\\boldsymbol{s}", "mbt": "\\boldsymbol{t}", "mbu": "\\boldsymbol{u}", "mbv": "\\boldsymbol{v}", "mbw": "\\boldsymbol{w}", "mbx": "\\boldsymbol{x}", "mby": "\\boldsymbol{y}", "mbz": "\\boldsymbol{z}", "mbA": "\\boldsymbol{A}", "mbB": "\\boldsymbol{B}", "mbC": "\\boldsymbol{C}", "mbD": "\\boldsymbol{D}", "mbE": "\\boldsymbol{E}", "mbF": "\\boldsymbol{F}", "mbG": "\\boldsymbol{G}", "mbH": "\\boldsymbol{H}", "mbI": "\\boldsymbol{I}", "mbJ": "\\boldsymbol{J}", "mbK": "\\boldsymbol{K}", "mbL": "\\boldsymbol{L}", "mbM": "\\boldsymbol{M}", "mbN": "\\boldsymbol{N}", "mbO": "\\boldsymbol{O}", "mbP": "\\boldsymbol{P}", "mbQ": "\\boldsymbol{Q}", "mbR": "\\boldsymbol{R}", "mbS": "\\boldsymbol{S}", "mbT": "\\boldsymbol{T}", "mbU": "\\boldsymbol{U}", "mbV": "\\boldsymbol{V}", "mbW": "\\boldsymbol{W}", "mbX": "\\boldsymbol{X}", "mbY": "\\boldsymbol{Y}", "mbZ": "\\boldsymbol{Z}", "bbA": "\\mathbb{A}", "bbB": "\\mathbb{B}", "bbC": "\\mathbb{C}", "bbD": "\\mathbb{D}", "bbE": "\\mathbb{E}", "bbG": "\\mathbb{G}", "bbH": "\\mathbb{H}", "bbI": "\\mathbb{I}", "bbJ": "\\mathbb{J}", "bbK": "\\mathbb{K}", "bbL": "\\mathbb{L}", "bbM": "\\mathbb{M}", "bbN": "\\mathbb{N}", "bbO": "\\mathbb{O}", "bbP": "\\mathbb{P}", "bbQ": "\\mathbb{Q}", "bbR": "\\mathbb{R}", "bbS": "\\mathbb{S}", "bbT": "\\mathbb{T}", "bbU": "\\mathbb{U}", "bbV": "\\mathbb{V}", "bbW": "\\mathbb{W}", "bbX": "\\mathbb{X}", "bbY": "\\mathbb{Y}", "bbZ": "\\mathbb{Z}", "cA": "\\mathcal{A}", "cB": "\\mathcal{B}", "cC": "\\mathcal{C}", "cD": "\\mathcal{D}", "cE": "\\mathcal{E}", "cG": "\\mathcal{G}", "cH": "\\mathcal{H}", "cI": "\\mathcal{I}", "cJ": "\\mathcal{J}", "cK": "\\mathcal{K}", "cL": "\\mathcal{L}", "cM": "\\mathcal{M}", "cN": "\\mathcal{N}", "cO": "\\mathcal{O}", "cP": "\\mathcal{P}", "cQ": "\\mathcal{Q}", "cR": "\\mathcal{R}", "cS": "\\mathcal{S}", "cT": "\\mathcal{T}", "cU": "\\mathcal{U}", "cV": "\\mathcal{V}", "cW": "\\mathcal{W}", "cX": "\\mathcal{X}", "cY": "\\mathcal{Y}", "cZ": "\\mathcal{Z}", "mbalpha": "\\boldsymbol{\\alpha}", "mbbeta": "\\boldsymbol{\\beta}", "mbgamma": "\\boldsymbol{\\gamma}", "mbdelta": "\\boldsymbol{\\delta}", "mbepsilon": "\\boldsymbol{\\epsilon}", "mbchi": "\\boldsymbol{\\chi}", "mbeta": "\\boldsymbol{\\eta}", "mbiota": "\\boldsymbol{\\iota}", "mbkappa": "\\boldsymbol{\\kappa}", "mblambda": "\\boldsymbol{\\lambda}", "mbmu": "\\boldsymbol{\\mu}", "mbnu": "\\boldsymbol{\\nu}", "mbomega": "\\boldsymbol{\\omega}", "mbtheta": "\\boldsymbol{\\theta}", "mbphi": "\\boldsymbol{\\phi}", "mbpi": "\\boldsymbol{\\pi}", "mbpsi": "\\boldsymbol{\\psi}", "mbrho": "\\boldsymbol{\\rho}", "mbsigma": "\\boldsymbol{\\sigma}", "mbtau": "\\boldsymbol{\\tau}", "mbupsilon": "\\boldsymbol{\\upsilon}", "mbxi": "\\boldsymbol{\\xi}", "mbzeta": "\\boldsymbol{\\zeta}", "mbvarepsilon": "\\boldsymbol{\\varepsilon}", "mbvarphi": "\\boldsymbol{\\varphi}", "mbvartheta": "\\boldsymbol{\\vartheta}", "mbvarrho": "\\boldsymbol{\\varrho}", "mbDelta": "\\boldsymbol{\\Delta}", "mbGamma": "\\boldsymbol{\\Gamma}", "mbLambda": "\\boldsymbol{\\Lambda}", "mbOmega": "\\boldsymbol{\\Omega}", "mbPhi": "\\boldsymbol{\\Phi}", "mbPsi": "\\boldsymbol{\\Psi}", "mbPi": "\\boldsymbol{\\Pi}", "mbSigma": "\\boldsymbol{\\Sigma}", "mbTheta": "\\boldsymbol{\\Theta}", "mbUpsilon": "\\boldsymbol{\\Upsilon}", "mbXi": "\\boldsymbol{\\Xi}", "mbzero": "\\boldsymbol{0}", "mbone": "\\boldsymbol{1}", "iid": ["\\stackrel{\\text{iid}}{#1}", 1], "ind": ["\\stackrel{\\text{ind}}{#1}", 1], "dif": "\\mathop{}\\!\\mathrm{d}", "diag": "\\textrm{diag}", "supp": "\\textrm{supp}", "Tr": "\\textrm{Tr}", "E": "\\mathbb{E}", "Var": "\\textrm{Var}", "Cov": "\\textrm{Cov}", "reals": "\\mathbb{R}", "naturals": "\\mathbb{N}", "KL": ["D_{\\textrm{KL}}\\left(#1\\;\\|\\;#2\\right)", 2]}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lectures/10_hmms';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Variational Autoencoders" href="11_vaes.html" />
    <link rel="prev" title="Mixture Models and EM" href="09_mixtures.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
  
    <p class="title logo__title">STATS 305B: Models and Algorithms for Discrete Data</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Overview
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Notes</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01_distributions.html">Discrete Distributions and the Basics of Statistical Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_contingency_tables.html">Contingency Tables</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_logreg.html">Logistic Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="04_expfam.html">Exponential Families</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_glms.html">Generalized Linear Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="06_bayes.html">Bayesian Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_bayes_glms_soln.html">Bayesian GLMs</a></li>
<li class="toctree-l1"><a class="reference internal" href="08_sparse_glms.html">Sparse GLMs</a></li>
<li class="toctree-l1"><a class="reference internal" href="09_mixtures.html">Mixture Models and EM</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Hidden Markov Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="11_vaes.html">Variational Autoencoders</a></li>
<li class="toctree-l1"><a class="reference internal" href="11_vaes_demo.html">Demo: Neural Networks and VAEs</a></li>
<li class="toctree-l1"><a class="reference internal" href="12_rnns.html">Recurrent Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="13_transformers.html">Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="15_graphs.html">Random Graphs Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="16_diffusion.html">Denoising Diffusion Models</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Assignments</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../assignments/hw0/hw0.html">HW0: PyTorch Primer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../assignments/hw1/hw1.html">HW1: Logistic Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../assignments/hw2/hw2.html">HW2: Bayesian GLMs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../assignments/hw3/hw3.html">HW3: Hidden Markov Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../assignments/hw4/hw4.html">HW4: Large Language Models</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">References</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="99_references.html">References</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/lectures/10_hmms.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Hidden Markov Models</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gaussian-mixture-models">Gaussian Mixture Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-em-algorithm">The EM algorithm</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Hidden Markov Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#inference-and-learning-in-hmm">Inference and Learning in HMM</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#computing-the-predictive-distributions">Computing the predictive distributions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#computing-the-posterior-marginal-distributions">Computing the posterior marginal distributions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#computing-the-backward-messages">Computing the backward messages</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-do-the-backward-messages-represent">What do the backward messages represent?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#computing-the-posterior-pairwise-marginals">Computing the posterior pairwise marginals</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#normalizing-the-messages-for-numerical-stability">Normalizing the messages for numerical stability</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#em-for-hidden-markov-models">EM for Hidden Markov Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="hidden-markov-models">
<h1>Hidden Markov Models<a class="headerlink" href="#hidden-markov-models" title="Link to this heading">#</a></h1>
<p><strong>Hidden Markov Models (HMMs)</strong> are latent variable models for sequential data. Like the mixture models from the previous chapter, HMMs have discrete latent states. Unlike mixture models, the discrete latent states of an HMM are not independent: the state at time <span class="math notranslate nohighlight">\(t\)</span> depends on the state at time <span class="math notranslate nohighlight">\(t-1\)</span>. These dependencies allow the HMM to capture how states transition from one to another over time. Thanks to the <strong>Markovian</strong> dependency structure, posterior inference and parameter estimation in HMMs remain tractable.</p>
<section id="gaussian-mixture-models">
<h2>Gaussian Mixture Models<a class="headerlink" href="#gaussian-mixture-models" title="Link to this heading">#</a></h2>
<p>Recall the basic Gaussian mixture model,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    z_t &amp;\stackrel{\text{iid}}{\sim} \mathrm{Cat}(\mbpi) \\
    x_t  \mid z_t &amp;\sim \mathcal{N}(\mbmu_{z_t}, \mbSigma_{z_t})
\end{aligned}
\end{split}\]</div>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(z_t \in \{1,\ldots,K\}\)</span> is a <strong>latent mixture assignment</strong></p></li>
<li><p><span class="math notranslate nohighlight">\(x_t \in \reals^D\)</span> is an <strong>observed data point</strong></p></li>
<li><p><span class="math notranslate nohighlight">\(\mbpi\in \Delta_{K-1}\)</span>,
<span class="math notranslate nohighlight">\(\mbmu_k \in \reals^D\)</span>, and
<span class="math notranslate nohighlight">\(\mbSigma_k \in \reals_{\succeq 0}^{D \times D}\)</span> are
parameters</p></li>
</ul>
<p>(Here we’ve switched to indexing data points by <span class="math notranslate nohighlight">\(t\)</span> rather than <span class="math notranslate nohighlight">\(n\)</span>.)</p>
<p>Let <span class="math notranslate nohighlight">\(\mbTheta\)</span> denote the set of parameters. We can be
Bayesian and put a prior on <span class="math notranslate nohighlight">\(\mbTheta\)</span> and run Gibbs or VI,
or we can point estimate <span class="math notranslate nohighlight">\(\mbTheta\)</span> with EM, etc.</p>
<div class="tip admonition">
<p class="admonition-title">Exercise</p>
<p>Draw the graphical model for a GMM.</p>
</div>
</section>
<section id="the-em-algorithm">
<h2>The EM algorithm<a class="headerlink" href="#the-em-algorithm" title="Link to this heading">#</a></h2>
<p>Recall the EM algorithm for mixture models,</p>
<ul>
<li><p><strong>E step:</strong> Compute the posterior distribution</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{aligned}
                q(\mbz_{1:T})
                &amp;= p(\mbz_{1:T}  \mid \mbx_{1:T}; \mbTheta) \\
                &amp;= \prod_{t=1}^T p(z_t  \mid \mbx_t; \mbTheta) \\
                &amp;= \prod_{t=1}^T q_t(z_t)
    \end{aligned}
    \end{split}\]</div>
</li>
<li><p><strong>M step:</strong> Maximize the ELBO wrt <span class="math notranslate nohighlight">\(\mbTheta\)</span>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{aligned}
                \mathcal{L}(\mbTheta)
                &amp;= \mathbb{E}_{q(\mbz_{1:T})}\left[\log p(\mbx_{1:T}, \mbz_{1:T}; \mbTheta) - \log q(\mbz_{1:T}) \right] \\
                &amp;= \mathbb{E}_{q(\mbz_{1:T})}\left[\log p(\mbx_{1:T}, \mbz_{1:T}; \mbTheta) \right]  + c.
            \end{aligned}
    \end{split}\]</div>
</li>
</ul>
<p>For exponential family mixture models, the M-step only requires expected
sufficient statistics.</p>
</section>
<section id="id1">
<h2>Hidden Markov Models<a class="headerlink" href="#id1" title="Link to this heading">#</a></h2>
<p>Hidden Markov Models (HMMs) are like mixture models
with temporal dependencies between the mixture assignments.</p>
<!-- ::: {.center}
![image](figures/lap7/hmm.png){width="70%"}
::: -->
<p>This graphical model says that the joint distribution factors as,</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
    p(z_{1:T}, \mbx_{1:T}) &amp;= p(z_1) \prod_{t=2}^T p(z_t  \mid z_{t-1}) \prod_{t=1}^T p(\mbx_t  \mid z_t).\end{aligned}
\]</div>
<p>We call this an HMM because the <em>hidden</em> states follow a Markov chain,
<span class="math notranslate nohighlight">\(p(z_1) \prod_{t=2}^T p(z_t  \mid z_{t-1})\)</span>.</p>
<p>An HMM consists of three components:</p>
<ol class="arabic simple">
<li><p><strong>Initial distribution:</strong>
<span class="math notranslate nohighlight">\(z_1 \sim \mathrm{Cat}(\mbpi_0)\)</span></p></li>
<li><p><strong>Transition matrix:</strong>
<span class="math notranslate nohighlight">\(z_t \sim \mathrm{Cat}(\mbP_{z_{t-1}})\)</span> where
<span class="math notranslate nohighlight">\(\mbP\in [0,1]^{K \times K}\)</span> is a <em>row-stochastic</em>
transition matrix with rows <span class="math notranslate nohighlight">\(\mbP_k\)</span>.</p></li>
<li><p><strong>Emission distribution:</strong>
<span class="math notranslate nohighlight">\(\mbx_t \sim p(\cdot  \mid \boldsymbol{\theta}_{z_t})\)</span></p></li>
</ol>
</section>
<section id="inference-and-learning-in-hmm">
<h2>Inference and Learning in HMM<a class="headerlink" href="#inference-and-learning-in-hmm" title="Link to this heading">#</a></h2>
<p>We are interested in questions like:</p>
<ul class="simple">
<li><p>What are the <em>predictive distributions</em> of
<span class="math notranslate nohighlight">\(p(z_{t+1}  \mid \mbx_{1:t})\)</span>?</p></li>
<li><p>What is the <em>posterior marginal</em> distribution
<span class="math notranslate nohighlight">\(p(z_t  \mid \mbx_{1:T})\)</span>?</p></li>
<li><p>What is the <em>posterior pairwise marginal</em> distribution
<span class="math notranslate nohighlight">\(p(z_t, z_{t+1}  \mid \mbx_{1:T})\)</span>?</p></li>
<li><p>What is the <em>posterior mode</em>
<span class="math notranslate nohighlight">\(z_{1:T}^\star = \mathop{\mathrm{arg\,max}}p(z_{1:T}  \mid \mbx_{1:T})\)</span>?</p></li>
<li><p>How can we <em>sample the posterior</em>
<span class="math notranslate nohighlight">\(p(\mbz_{1:T}  \mid \mbx_{1:T})\)</span> of an HMM?</p></li>
<li><p>What is the <em>marginal likelihood</em> <span class="math notranslate nohighlight">\(p(\mbx_{1:T})\)</span>?</p></li>
<li><p>How can we <em>learn the parameters</em> of an HMM?</p></li>
</ul>
<div class="tip admonition">
<p class="admonition-title">Exercise</p>
<p>On the surface, what makes these inference problems harder than in the simple mixture model case?</p>
</div>
</section>
<section id="computing-the-predictive-distributions">
<h2>Computing the predictive distributions<a class="headerlink" href="#computing-the-predictive-distributions" title="Link to this heading">#</a></h2>
<p>The predictive distributions give the probability of the latent state <span class="math notranslate nohighlight">\(z_{t+1}\)</span> given observations <em>up to but not including</em> time <span class="math notranslate nohighlight">\(t+1\)</span>. Let,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
        \alpha_{t+1}(z_{t+1}) &amp;\triangleq p(z_{t+1}, \mbx_{1:t}) \\
        &amp;= \sum_{z_1=1}^K \cdots \sum_{z_{t}=1}^K p(z_1) \prod_{s=1}^{t} p(\mbx_s  \mid z_s) \, p(z_{s+1}  \mid z_{s})\\
        &amp;= \sum_{z_{t}=1}^K \left[ \left( \sum_{z_1=1}^K \cdots \sum_{z_{t-1}=1}^K p(z_1) \prod_{s=1}^{t-1} p(\mbx_s  \mid z_s) \, p(z_{s+1}  \mid z_{s}) \right) p(\mbx_t  \mid z_t) \, p(z_{t+1}  \mid z_{t}) \right]  \\
        &amp;= \sum_{z_{t}=1}^K \alpha_t(z_t) \, p(\mbx_t  \mid z_t) \, p(z_{t+1}  \mid z_{t}).
\end{aligned}
\end{split}\]</div>
<p>We call <span class="math notranslate nohighlight">\(\alpha_t(z_t)\)</span> the <em>forward messages</em>. We
can compute them recursively! The base case is
<span class="math notranslate nohighlight">\(p(z_1  \mid \varnothing) \triangleq p(z_1)\)</span>.</p>
<p>We can also write these
recursions in a vectorized form. Let</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
        \mbalpha_t &amp;=
        \begin{bmatrix}
        \alpha_t(z_t = 1) \\
        \vdots \\
        \alpha_t(z_t=K)
        \end{bmatrix}
        =
        \begin{bmatrix}
        p(z_t = 1, \mbx_{1:t-1}) \\
        \vdots \\
        p(z_t = K, \mbx_{1:t-1})
        \end{bmatrix}
        \qquad \text{and} \qquad
        \mbl_t =
        \begin{bmatrix}
        p(\mbx_t  \mid z_t = 1) \\
        \vdots \\
        p(\mbx_t  \mid z_t = K)
        \end{bmatrix}
    \end{aligned}
\end{split}\]</div>
<p>both be vectors in <span class="math notranslate nohighlight">\(\reals_+^K\)</span>. Then,</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
        \mbalpha_{t+1} &amp;= \mbP^\top (\mbalpha_t \odot \mbl_t)
    \end{aligned}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\odot\)</span> denotes the Hadamard (elementwise)
product and <span class="math notranslate nohighlight">\(\mbP\)</span> is the transition matrix.</p>
<p>Finally, to get the
predictive distributions we just have to normalize,</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
        p(z_{t+1}  \mid \mbx_{1:t}) &amp;\propto p(z_{t+1}, \mbx_{1:t}) = \alpha_{t+1}(z_{t+1}).
    \end{aligned}
\]</div>
<div class="tip admonition">
<p class="admonition-title">Question</p>
<p>What does the normalizing constant
tell us?</p>
</div>
</section>
<section id="computing-the-posterior-marginal-distributions">
<h2>Computing the posterior marginal distributions<a class="headerlink" href="#computing-the-posterior-marginal-distributions" title="Link to this heading">#</a></h2>
<p>The posterior marginal
distributions give the probability of the latent state <span class="math notranslate nohighlight">\(z_{t}\)</span> given
<em>all the observations</em> up to time <span class="math notranslate nohighlight">\(T\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
        p(z_{t}  \mid \mbx_{1:T})
        &amp;\propto \sum_{z_1=1}^K \cdots \sum_{z_{t-1}=1}^K \sum_{z_{t+1}=1}^K \cdots \sum_{z_T=1}^K p(\mbz_{1:T}, \mbx_{1:T}) \\
        &amp;= \nonumber
        \bigg[ \sum_{z_{1}=1}^K \cdots \sum_{z_{t-1}=1}^K p(z_1) \prod_{s=1}^{t-1} p(\mbx_s  \mid z_s) \, p(z_{s+1}  \mid z_{s}) \bigg]
        \times  p(\mbx_t  \mid z_t)   \\
        &amp;\qquad \times \bigg[ \sum_{z_{t+1}=1}^K \cdots \sum_{z_T=1}^K \prod_{u=t+1}^T p(z_{u}  \mid z_{u-1}) \, p(\mbx_u  \mid z_u) \bigg] \\
        &amp;= \alpha_t(z_t) \times p(\mbx_t  \mid z_t) \times \beta_t(z_t)
    \end{aligned}
\end{split}\]</div>
<p>where we have introduced the <em>backward messages</em>
<span class="math notranslate nohighlight">\(\beta_t(z_t)\)</span>.</p>
</section>
<section id="computing-the-backward-messages">
<h2>Computing the backward messages<a class="headerlink" href="#computing-the-backward-messages" title="Link to this heading">#</a></h2>
<p>The backward messages can be computed
recursively too,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
        \beta_t(z_t)
        &amp;\triangleq \sum_{z_{t+1}=1}^K \cdots \sum_{z_T=1}^K \prod_{u=t+1}^T p(z_{u}  \mid z_{u-1}) \, p(\mbx_u  \mid z_u) \\
        &amp;= \sum_{z_{t+1}=1}^K p(z_{t+1}  \mid z_t) \, p(\mbx_{t_1}  \mid z_{t+1}) \left(\sum_{z_{t+2}=1}^K \cdots \sum_{z_T=1}^K \prod_{u=t+2}^T p(z_{u}  \mid z_{u-1}) \, p(\mbx_u  \mid z_u) \right) \\
        &amp;= \sum_{z_{t+1}=1}^K p(z_{t+1}  \mid z_t) \, p(\mbx_{t_1}  \mid z_{t+1}) \, \beta_{t+1}(z_{t+1}).
    \end{aligned}
\end{split}\]</div>
<p>For the base case, let <span class="math notranslate nohighlight">\(\beta_T(z_T) = 1\)</span>.</p>
<p>Let</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
        \boldsymbol{\beta}_t &amp;=
        \begin{bmatrix}
        \beta_t(z_t = 1) \\
        \vdots \\
        \beta_t(z_t=K)
        \end{bmatrix}
    \end{aligned}
\end{split}\]</div>
<p>be a vector in <span class="math notranslate nohighlight">\(\reals_+^K\)</span>. Then,</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
        \boldsymbol{\beta}_{t} &amp;= \mbP(\boldsymbol{\beta}_{t+1} \odot \mbl_{t+1}).
    \end{aligned}
\]</div>
<p>Let <span class="math notranslate nohighlight">\(\boldsymbol{\beta}_T = \boldsymbol{1}_K\)</span>.</p>
<p>Now we have everything we need to compute the posterior marginal,</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
        p(z_t = k  \mid \mbx_{1:T}) &amp;= \frac{\alpha_{t,k} \, l_{t,k} \, \beta_{t,k}}{\sum_{j=1}^K \alpha_{t,j} l_{t,j} \beta_{t,j}}.
    \end{aligned}
\]</div>
<p>We just derived the <strong>forward-backward algorithm</strong> for HMMs!</p>
<section id="what-do-the-backward-messages-represent">
<h3>What do the backward messages represent?<a class="headerlink" href="#what-do-the-backward-messages-represent" title="Link to this heading">#</a></h3>
<div class="tip admonition">
<p class="admonition-title">Exercise</p>
<p>If the forward
messages represent the predictive probabilities
<span class="math notranslate nohighlight">\(\alpha_{t+1}(z_{t+1}) = p(z_{t+1}, \mbx_{1:t})\)</span>, what do the
backward messages represent?</p>
</div>
</section>
</section>
<section id="computing-the-posterior-pairwise-marginals">
<h2>Computing the posterior pairwise marginals<a class="headerlink" href="#computing-the-posterior-pairwise-marginals" title="Link to this heading">#</a></h2>
<div class="tip admonition">
<p class="admonition-title">Exercise</p>
<p>Use the forward
and backward messages to compute the posterior pairwise marginals
<span class="math notranslate nohighlight">\(p(z_t, z_{t+1}  \mid \mbx_{1:T})\)</span>.</p>
</div>
</section>
<section id="normalizing-the-messages-for-numerical-stability">
<h2>Normalizing the messages for numerical stability<a class="headerlink" href="#normalizing-the-messages-for-numerical-stability" title="Link to this heading">#</a></h2>
<p>If you’re working with
long time series, especially if you’re working with 32-bit floating
point, you need to be careful.</p>
<p>The messages involve products of probabilities, which can quickly
overflow.</p>
<p>There’s a simple fix though: after each step, re-normalize the messages
so that they sum to one. I.e replace</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
        \mbalpha_{t+1} &amp;= \mbP^\top (\mbalpha_t \odot \mbl_t)
\end{aligned}
\]</div>
<p>with</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
        \overline{\mbalpha}_{t+1} &amp;= \frac{1}{A_t} \mbP^\top (\overline{\mbalpha}_t \odot \mbl_t) \\
        A_t &amp;= \sum_{k=1}^K \sum_{j=1}^K P_{jk} \overline{\alpha}_{t,j} l_{t,j}
            \equiv \sum_{j=1}^K \overline{\alpha}_{t,j} l_{t,j} \quad \text{(since $\mbP$ is row-stochastic)}.
\end{aligned}
\end{split}\]</div>
<p>This leads to a nice interpretation: The normalized
messages are predictive likelihoods
<span class="math notranslate nohighlight">\(\overline{\alpha}_{t+1,k} = p(z_{t+1}=k  \mid \mbx_{1:t})\)</span>,
and the normalizing constants are
<span class="math notranslate nohighlight">\(A_t = p(\mbx_t  \mid \mbx_{1:t-1})\)</span>.</p>
</section>
<section id="em-for-hidden-markov-models">
<h2>EM for Hidden Markov Models<a class="headerlink" href="#em-for-hidden-markov-models" title="Link to this heading">#</a></h2>
<p>Now we can put it all together. To perform EM in an HMM,</p>
<ul>
<li><p><strong>E step:</strong> Compute the posterior distribution</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
                q(\mbz_{1:T})
                &amp;= p(\mbz_{1:T}  \mid \mbx_{1:T}; \mbTheta).
    \end{align*}\]</div>
<p>(Really, run the <strong>forward-backward algorithm</strong> to get posterior marginals and pairwise marginals.)</p>
</li>
<li><p><strong>M step:</strong> Maximize the ELBO wrt <span class="math notranslate nohighlight">\(\mbTheta\)</span>,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
                \mathcal{L}(\mbTheta)
                &amp;= \mathbb{E}_{q(\mbz_{1:T})}\left[\log p(\mbx_{1:T}, \mbz_{1:T}; \mbTheta) \right]  + c \\
                &amp;= \nonumber \mathbb{E}_{q(\mbz_{1:T})}\left[\sum_{k=1}^K \mathbb{I}[z_1=k]\log \pi_{0,k} \right] +
                \mathbb{E}_{q(\mbz_{1:T})}\left[\sum_{t=1}^{T-1} \sum_{i=1}^K \sum_{j=1}^K \mathbb{I}[z_t=i, z_{t+1}=j]\log P_{i,j} \right] \\
                &amp;\qquad + \mathbb{E}_{q(\mbz_{1:T})}\left[\sum_{t=1}^T \sum_{k=1}^K \mathbb{I}[z_t=k]\log p(\mbx_t; \theta_k) \right]
    \end{align*}\]</div>
</li>
</ul>
<p>For exponential family observations, the M-step only requires expected
sufficient statistics.</p>
<div class="tip admonition">
<p class="admonition-title">Questions</p>
<ul class="simple">
<li><p>How can we sample the posterior?</p></li>
<li><p>How can we find the posterior mode?</p></li>
<li><p>How can we choose the number of states?</p></li>
<li><p>What if my transition matrix is sparse?</p></li>
</ul>
</div>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading">#</a></h2>
<p>HMMs add temporal dependencies to the latent states of mixture models. They’re a simple yet powerful model for sequential data.</p>
<p>The emission distribution can be extended in many ways. For example, we could include temporal dependencies in the emissions via an autoregressive HMM, or condition on external covariates as in an input-output HMM</p>
<p>Like mixture models, we can derive efficient <strong>stochastic EM</strong> algorithms for HMMs, which keep rolling averages of sufficient statistics across mini-batches (e.g., individual trials from a collection of sequences).</p>
<p>It’s always good to implement models and algorithms yourself at least once, like you will in Homework 3. Going forward, you should check out our implementations in <a class="reference external" href="https://probml.github.io/dynamax/">Dynamax</a> and <a class="reference external" href="https://lindermanlab.github.io/ssm-docs/">SSM</a>!</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./lectures"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="09_mixtures.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Mixture Models and EM</p>
      </div>
    </a>
    <a class="right-next"
       href="11_vaes.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Variational Autoencoders</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gaussian-mixture-models">Gaussian Mixture Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-em-algorithm">The EM algorithm</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Hidden Markov Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#inference-and-learning-in-hmm">Inference and Learning in HMM</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#computing-the-predictive-distributions">Computing the predictive distributions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#computing-the-posterior-marginal-distributions">Computing the posterior marginal distributions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#computing-the-backward-messages">Computing the backward messages</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-do-the-backward-messages-represent">What do the backward messages represent?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#computing-the-posterior-pairwise-marginals">Computing the posterior pairwise marginals</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#normalizing-the-messages-for-numerical-stability">Normalizing the messages for numerical stability</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#em-for-hidden-markov-models">EM for Hidden Markov Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Scott Linderman
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>